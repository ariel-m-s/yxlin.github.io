<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | Cognitive Models</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dynamic Models of Choice with Better Graphic Tools and Quicker Computations" />
<meta property="og:description" content="Dynamic Models of Choice with Better Graphic Tools and Quicker Computations" />
<link rel="canonical" href="http://localhost:4000/search/" />
<meta property="og:url" content="http://localhost:4000/search/" />
<meta property="og:site_name" content="Cognitive Models" />
<script type="application/ld+json">
{"@type":"WebPage","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/siteicon.png"}},"url":"http://localhost:4000/search/","headline":"Search","description":"Dynamic Models of Choice with Better Graphic Tools and Quicker Computations","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Cognitive Models" />

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="/css/main.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">

		
	</head>

	<body>
	  <header>
	    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
	    
			<h1>
				<a href="/"><img src="/images/emblem.svg" width="40" height="40" alt="Cognitive Models logo"></a>
				Cognitive Models
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="/search/" method="get">
				<input type="text" name="q" id="search-input" placeholder="Search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="/"></a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="/BUGS/hnormal/">BUGS Examples Volumn 1</a>
							<ul>
								
									<li class="nav-item "><a href="/BUGS/hnormal/">Hierarchical Normal Model</a></li>
								
									<li class="nav-item "><a href="/BUGS/seeds/">Random effect logistic regression</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/approximation/kde/">Likelihood Free Method</a>
							<ul>
								
									<li class="nav-item "><a href="/approximation/kde/">Kernel Density Estimation</a></li>
								
									<li class="nav-item "><a href="/approximation/pda/">Probability Density Approximation</a></li>
								
									<li class="nav-item "><a href="/approximation/ppda/">Parallel Probability Density Approximation</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/basics/model_array/">Modelling Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/basics/model_array/">Model Array</a></li>
								
									<li class="nav-item "><a href="/basics/simulation/">Simulation</a></li>
								
									<li class="nav-item "><a href="/basics/descriptive/">Descriptive Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/summary/">Summary Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/mle/">Maximising Likelihoods</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/bayes-basics/theorem/">Bayesian Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/bayes-basics/theorem/">Bayes' Theorem</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/prior/">Prior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/likelihood/">Model Likelihood</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/posterior/">Posterior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/diagnosis/">Checking Fitted Models</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/cognitive-model/sdt/">Cognitive Model</a>
							<ul>
								
									<li class="nav-item "><a href="/cognitive-model/sdt/">Signal Detection Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba/">Linear Ballistic Accumulation Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm/">Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/cddm/">Circular Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/plba/">PLBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pddm/">PDDM</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba3/">Three-accumulator LBA Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/fixed-effect-model/one_participant/">Fixed Effects Model</a>
							<ul>
								
									<li class="nav-item "><a href="/fixed-effect-model/one_participant/">One Participant</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/many_participants/">Multiple Participants</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/mcmc/mcmc/">Modern Bayesian Statistics</a>
							<ul>
								
									<li class="nav-item "><a href="/mcmc/mcmc/">Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/mcmc/rwm/">Random Walk Metropolis</a></li>
								
									<li class="nav-item "><a href="/mcmc/hastings/">Metropolis-Hastings</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/random-effect-model/hlba/">Hierarchical Model</a>
							<ul>
								
									<li class="nav-item "><a href="/random-effect-model/hlba/">Hierarchical LBA Model</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hddm/">Hierarchical DDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hcddm/">Hierarchical Circular DDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision1/">Shooting Decision Model - Recovery Study</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision2/">Shooting Decision Model - Empirical Data</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/sampling/genetic/">Sampling Techniques</a>
							<ul>
								
									<li class="nav-item "><a href="/sampling/genetic/">Population-based Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/crossover/">Crossover</a></li>
								
									<li class="nav-item "><a href="/sampling/migration/">Migration</a></li>
								
									<li class="nav-item "><a href="/sampling/mutation/">Mutation</a></li>
								
									<li class="nav-item "><a href="/sampling/demc/">Differential Evolution Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/demcmc/">Differential Evolution Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/dgmc/">Distributed Genetic Monte Carlo</a></li>
								
							</ul>
						</li>
					
				</ul>

				<ul>
					<li class="nav-item top-level ">
						
						<a href="/changelog/">Change Log</a>
					</li>
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>Cognitive Models</h2>
				<h3>Search</h3>
			</div>
			<article class="content">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					

					"bugs-hnormal": {
						"id": "bugs-hnormal",
						"title": "Hierarchical Normal Model",
						"category": "",
						"url": " /BUGS/hnormal/",
						"content": "Disclaimer: This tutorial uses an experimental (beta) version of ggdmc, which has added the functionality of fitting regression models. The software can be found in its GitHub. The aim of tutorial is to docuemnt one method to fit an hierarchical normal model, using the Rats data. Rats data were studied in Gelfand (1990) and used in the BUGS examples volumn I. This expands the scope of ggdmc, not only to fit cognitive models but also to fit standard regression models. I first convert the data from wide to long format. setwd(\"~ BUGS_Examples vol1 Rats \") tmp &lt;- dget(\"data dataBUGS.R\") d &lt;- data.frame(matrix(as.vector(tmp$Y), nrow = 30, byrow = TRUE)) names(d) &lt;- c(8, 15, 22, 29, 36) d$s &lt;- factor(1:tmp$N) long &lt;- melt(d, id.vars = c(\"s\"), variable.name = \"xfac\", value.name = \"RT\") dplyr::tbl_df(long) long$X &lt;- as.double(as.character(long$xfac)) - tmp$xbar long$S &lt;- factor(\"x1\") long$R &lt;- factor(\"r1\") d &lt;- long[, c(\"s\", \"S\", \"R\", \"X\", \"RT\")] The data can be visualized as many lines, each representing a subject (rat). p1 &lt;- ggplot(d1, aes(x = X, y = RT, group = s, colour = s)) + geom_line(size = 1) + geom_point() + ylab(\"Weight\") + ggtitle(\"Complete data\") + coord_cartesian(ylim = c(120, 380)) + scale_colour_grey(na.value = \"black\") + theme_bw(base_size = 20) + theme(legend.position = \"none\") Set-up Model The DDM composes of two complementary defective distribtions; thereby, two response types. Unlike the DDM, a regression model has only one response type; that is one (complete) distribution. Therefore, the match.map and responses arguments are set as NULL and “r1” (meaning only one response type). The argument regressors enters the independent predictive variable (typically denoted X). In the Rats example, the weights of thirty young rats were measured weekly for five weeks and the measurements taken at the end of each week (8th, 15th, 22rd, 29th, &amp; 36th day) were provided. The parameterization is a (intercept), b (slope) and the precision (). require(ggdmc) model &lt;- BuildModel( p.map = list(a = \"1\", b = \"1\", tau = \"1\"), match.map = NULL, regressors = c(8, 15, 22, 29, 36) - tmp$xbar,, factors = list(S = c(\"x1\")), responses = \"r1\", constants = NULL, type = \"glm\") ## Parameter vector names are: ( see attr(,\"p.vector\") ) ## [1] \"a\" \"b\" \"tau\" ## ## Constants are (see attr(,\"constants\") ): ## NULL ## ## Model type = glm Recovery Study I take values from the Rats example as the true parameters at the population level and use them to simulate an ideal data set, which has 1000 rats and each of them contributes 100 response. tnorm2 is truncated normal distribution, using mean and precision parametrization. When both the upper and lower are set NA, the tnorm becomes normal distribution. npar &lt;- length(GetPNames(model)) pop.location &lt;- c(a = 242.7, b = 6.189, tau = .03) pop.scale &lt;- c(a = .005, b = 3.879, tau = .04) ntrial &lt;- 100 pop.prior &lt;-BuildPrior( dists = rep(\"tnorm2\", npar), p1 = pop.location, p2 = pop.scale, lower = c(NA, 0, 0), upper = rep(NA, npar)) dat &lt;- simulate(model, nsub = 1000, nsim = ntrial, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") ## Extract true parameters for each individual This plots the distributions that generate the simulation data and shows the location parameters of these distributions as dashed lines. plot(pop.prior, ps = pop.mean) Set up Priors To randomly draw initial values for the data- and hyper-level parameters, I set up three sets of distributions and bind them as a list, named start. These are used to generate start values only. pstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 242, b = 6.19, tau = .027), p2 = c(a = 14, b = .49, tau = 10), lower = c(NA, NA, 0), upper = rep(NA, npar)) lstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 200, b = 5, tau = .01), p2 = c(a = 50, b = 1, tau = .01), lower = c(NA, NA, 0), upper = rep(NA, npar)) sstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 10, b = .5, tau = .01), p2 = c(a = 5, b = .1, tau = .01), lower = c(NA, NA, 0), upper = rep(NA, npar)) start &lt;- list(pstart, lstart, sstart) Next, I set up the structure of the hierarchical model. It is important to understand the hierarchical structure, so I sketch a diagram to show how the codes of setting up the prior distributions associate with the model structure. p.prior &lt;- BuildPrior( dists = rep(\"tnorm2\", npar), p1 = c(a = NA, b = NA, tau = NA), ## the value are drawn from hyper-level p2 = rep(NA, 3), ## (mu and sigma) prior, so all set NA lower = c(NA, 0, 0), upper = rep(NA, npar)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm2\", npar), p1 = c(a = 200, b = 6, tau = 3) p2 = c(a = 1e-4, b = 1e-3, tau = 1e-2) lower = c(NA, 0, 0), upper = rep(NA, npar)) sigma.prior &lt;- BuildPrior( dists = rep(\"gamma\", npar), p1 = c(a = .01, b = .01, tau = .01), p2 = c(a = 1000, b = 1000, tau = 1000), lower = c(0, 0, 0), upper = rep(NA, npar)) prior &lt;- list(p.prior, mu.prior, sigma.prior) Sampling The function, StartNewhiersamples use the start priors only for drawing initial values. The prior distributions wil be used in the model fit. fit &lt;- run(StartNewhiersamples(5e2, dmi, start, prior)) fit &lt;- run(RestartHypersamples(5e2, fit, thin = 32)) Model Diagnosis As usually, I check the potential scale reduction factors (Brook &amp; Gelman,1998), effective sample sizes, and trace plots. rhat &lt;- hgelman(fit, verbose = TRUE) hes &lt;- effectiveSize(hsam, hyper = TRUE) es1 &lt;- effectiveSize(hsam[[1]]) ## a.h1 b.h1 tau.h1 a.h2 b.h2 tau.h2 ## 427.9587 767.7925 483.1228 613.3212 730.1198 462.4242 ## a b tau ## 569.3685 609.0303 572.1573 p0 &lt;- plot(fit, hyper = TRUE) p1 &lt;- plot(fit, hyper = TRUE, pll = FALSE, den = TRUE) est1 &lt;- summary(fit, hyper = TRUE, recover = TRUE, start = 101, ps = pop.mean, type = 1, verbose = TRUE, digits = 3) est2 &lt;- summary(fit, hyper = TRUE, recover = TRUE, start = 101, ps = pop.scale, type = 2, verbose = TRUE, digits = 3) est3 &lt;- summary(fit, recover = TRUE, ps = ps, verbose = TRUE) ## a b tau ## True 242.700 6.189 0.030 ## 2.5% Estimate 241.456 6.143 0.008 ## 50% Estimate 242.275 6.173 0.460 ## 97.5% Estimate 243.081 6.206 1.338 ## Median-True -0.425 -0.016 0.430 ## ## a b tau ## True 0.005 3.879 0.040 ## 2.5% Estimate 0.005 3.533 0.042 ## 50% Estimate 0.005 3.835 0.047 ## 97.5% Estimate 0.005 4.156 0.058 ## Median-True 0.000 -0.044 0.007 ## ## Summary each participant separately ## a b tau ## Mean 242.28 6.17 3.82 ## True 242.29 6.17 3.81 ## Diff 0.01 0.00 -0.01 ## Sd 14.14 0.51 2.80 ## True 14.13 0.51 2.91 ## Diff 0.00 0.00 0.11 Model Fit to Rats Data After verifying that the model structure is OK, I then fit the Rats data. ## Each rat contribute 5 trials observations DT &lt;- data.table(d) DT[, .N, .(s)] ## s N ## 1: 1 5 ## 2: 2 5 ## 3: 3 5 ## ... ## 30: 30 5 Now, I bind the Rats data with the model and start sampling. The estimates are fairly similar with BUGS and Stan estimations. The only significant difference is the tau.h1, which simply due the more complex structure used here. dmi &lt;- BuildDMI(d, model) fit0 &lt;- run(StartNewhiersamples(500, dmi, start, prior)) fit &lt;- run(RestartHypersamples(5e2, fit0, thin = 32)) est1 &lt;- summary(fit, hyper = TRUE, type = 1, verbose = TRUE) round( est1$quantiles, 3) # 2.5% 25% 50% 75% 97.5% # a.h1 237.37 240.69 242.49 244.12 247.75 # mu_alpha 237.09 240.68 242.47 244.29 247.84 ## Stan # alpha.c 237.10 240.90 242.70 244.50 248.10 ## BUGS # b.h1 5.98 6.10 6.17 6.24 6.37 # mu_beta 5.97 6.11 6.18 6.26 6.40 ## Stan # beta.c 5.97 6.12 6.19 6.26 6.40 ## BUGS # tau.h1 0.012 0.035 0.042 0.047 0.056 # Stan 0.020 0.024 0.027 0.030 0.036 # BUGS 0.020 0.024 0.027 0.030 0.036 # a.h2 0.003 0.004 0.005 0.006 0.008 # alpha.tau 0.003 0.004 0.005 0.006 0.008 ## BUGS # b.h2 2.049 3.111 3.838 4.683 6.714 # beta.tau 1.952 3.078 3.879 4.922 8.026 ## BUGS hes &lt;- effectiveSize(fit, hyper = TRUE) ## a.h1 b.h1 tau.h1 a.h2 b.h2 tau.h2 ## 4500.000 4534.287 4193.927 3807.489 4079.768 3139.033 round(apply(data.frame(es), 1, mean)) round(apply(data.frame(es), 1, sd)) round(apply(data.frame(es), 1, max)) round(apply(data.frame(es), 1, min)) ## a b tau ## Mean 4464 4360 4321 ## SD 139 242 348 ## MAX 4717 5020 5137 ## MIN 4100 3732 3455 Handling Missing Data Rats example also considered fitting missing data. This can be done by setting the missing data as NA or downloading the missing data set directly from BUGS site. d[6:10,5] &lt;- NA d[11:20,4:5] &lt;- NA d[21:25,3:5] &lt;- NA d[26:30,2:5] &lt;- NA and bind the data with the same model set up previously. Again, the results are fairly similar with using other Bayesian software. dmi &lt;- BuildDMI(d[!is.na(d$RT),], model) # 2.5% 25% 50% 75% 97.5% # a.h1 241.01 244.35 246.07 247.73 251.09 # alpha.c 240.30 243.90 245.80 247.70 251.30 BUGS # b.h1 6.362 6.526 6.605 6.688 6.844 # beta.c 6.286 6.477 6.572 6.669 6.870 BUGS # a.h2 0.003 0.004 0.005 0.006 0.009 # alpha.tau 0.003 0.004 0.005 0.006 0.009 BUGS # b.h2 1.640 2.757 3.595 4.479 7.241 # beta.tau 1.505 2.676 3.620 5.044 13.601 BUGS The predictions for the final four observations on rat 26 can be obtained by entering predict_one function with fit[[26]]. pp26 &lt;- predict_one(fit[[26]]) pred26 &lt;- pp26[, .(Mean = mean(RT)), .(X)] pred26[c(2,1,3,4,5),] ## X Mean ## 1: -14 160.8060 ## 2: -7 203.8068 Y[26, 2] = 204.6 ## 3: 0 249.4786 Y[26, 3] = 250.2 ## 4: 7 297.8309 Y[26, 4] = 295.6 ## 5: 14 339.6564 Y[26, 5] = 341.2 Reference Gelfand, A. E., Hills, S. E., Racine-Poon, A., &amp; Smith, A. F. (1990). Illustration of Bayesian inference in normal data models using Gibbs sampling. Journal of the American Statistical Association, 85(412), 972-985."
					}

					
				
			
		
			
				
					,
					

					"bugs-seeds": {
						"id": "bugs-seeds",
						"title": "Random effect logistic regression",
						"category": "",
						"url": " /BUGS/seeds/",
						"content": "Disclaimer: This tutorial uses an experimental (beta) version of ggdmc, which has added the functionality of fitting logistic regression models. The software can be found in its GitHub. This document has yet completed. The aim of tutorial is to document one method to fit the logistic regression model, using the Seeds data. Seeds data were studied in Crowder (1978), re-analysed by Breslow and Clayton (1993) and used in the BUGS examples volumn I. This document expands the scope of ggdmc to the logistic regression model. An ordinary logistic model can fit either binary (response) data (i.e., 0, 1, 0, …) or binomial data (i.e., proportional data, as the Seeds example). The simplest form of the random-effect (multilevel) logistic model is to presume observation units are drawn from a normal distribution. This two-level model can be compared to the model presuming observation units are as they been observed (i.e., fixed-effect logistic regression model). Here I use the formulation of anti-logit, because firstly it is easier to interpret the probability of success (i.e., ) and secondly it is practically how computer codes been implemented. The idea of transforming binary or binomial responses with logit is still conceptually important for the generalized linear model though. Because the Seeds data set was formatted as List, I convert the data to data frame format. rm(list = ls()) library(data.table); library(boot) ## Load Seeds data ------------ ## 2 x 2 design setwd(\"~ BUGS_Examples vol1 Seeds \") dat &lt;- list(S = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10, 8, 10, 8, 23, 0, 3, 22, 15, 32, 3), N = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7), ## seed variety; 0 = aegytpiao 75 1 = aegyptiao 73 X1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), ## root extract; 0 = bean; 1 = cucumber X2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1), Ns = 21) d &lt;- data.table(S = dat$S, N = dat$N, P = dat$S dat$N, X1= dat$X1, X2 = dat$X2) dplyr::tbl_df(d) d$s &lt;- factor(1:dat$Ns) ## A tibble: 21 x 7 ## S N P X1 X2 logit s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 10 39 0.256 0 0 -1.06 1 ## 2 23 62 0.371 0 0 -0.528 2 ## 3 23 81 0.284 0 0 -0.925 3 ## 4 26 51 0.510 0 0 0.0392 4 ## 5 17 39 0.436 0 0 -0.258 5 ## 6 5 6 0.833 0 1 1.61 6 ## 7 53 74 0.716 0 1 0.926 7 ## 8 55 72 0.764 0 1 1.17 8 ## 9 32 51 0.627 0 1 0.521 9 ## 10 46 79 0.582 0 1 0.332 10 ## ... with 11 more rows S, the number of (successfully) germinated seeds on the ith plate (i = 1, … N); N, the number of total seeds on the ith plate; P, the proportion of germinated seeds; X1, a two-level seed factor, aegyptiao 75 vs. aegyptiao 73; X2, a two-level root extract factor, bean vs. cucumber; logit, as the column name says; s, subject, namely, the observation unit. The interaction plot shows that the root extract type, cucumber, has a drastic increase in successful germination when the seed type is aegyptiao 75, comparing to when the seed type is aegyptaio 73 and this change is small and in an opposite direction in the root extract type, bean. The data can be analysed with the ordinary logistic regression (OLR) model or multilevel logistic regression model. The OLR replicates the result in Table 3 (1st column) in Breslow and Clayton (1993). I use the display function in the arm package, which shows summary result concisely (AIC is calculated separately). m1 &lt;- glm(cbind(S, N-S) ~ X1 + X2, family = binomial, data = d) arm::display(m1) ## Breslow's result in Table 3 p15 ## glm(formula = cbind(S, N - S) ~ X1 + X2, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -0.43 0.11 ## X1 -0.27 0.15 ## X2 1.06 0.14 ## --- ## n = 21, k = 3 ## residual deviance = 39.7, null deviance = 98.7 (difference = 59.0) ## AIC: 122.28 m2 &lt;- glm(cbind(S, N-S) ~ X1*X2, family = binomial, data = d) arm::display(m2) ## glm(formula = cbind(S, N - S) ~ X1 * X2, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -0.56 0.13 ## X1 0.15 0.22 ## X2 1.32 0.18 ## X1:X2 -0.78 0.31 ## --- ## n = 21, k = 4 ## residual deviance = 33.3, null deviance = 98.7 (difference = 65.4) ## AIC: 117.87 require(lme4) m3 &lt;- glmer(cbind(S, N - S) ~ X1 * X2 + (1 | s), family = binomial(link=\"logit\"), data = d) arm::display(m3) ## glmer(formula = cbind(S, N - S) ~ X1 * X2 + (1 | s), data = d, ## family = binomial(link = \"logit\")) ## coef.est coef.se ## (Intercept) -0.55 0.17 ## X1 0.10 0.28 ## X2 1.34 0.24 ## X1:X2 -0.81 0.38 ## ## Error terms: ## Groups Name Std.Dev. ## s (Intercept) 0.23 ## Residual 1.00 ## --- ## number of obs: 21, groups: s, 21 ## AIC = 117.5, DIC = -74.6 ## deviance = 16.5   glm   glmer   BUGS   ggdmc     se se se se -0.558 0.126 -0.548 0.166 -0.557 0.197     0.146 0.223 0.097 0.277 0.086 0.317     1.318 0.177 1.337 0.236 1.348 0.276     -0.778 0.306 -0.810 0.384 -0.824 0.445     — — 0.235 — 0.286 0.146     Reference Breslow, N. E., &amp; Clayton, D. G. (1993). Approximate inference in generalized linear mixed models. Journal of the American statistical Association, 88(421), 9-25. Crowder, M. J. (1978). Beta-binomial anova for proportions. Applied statistics, 34-37."
					}

					
				
			
		
			
				
					,
					

					"approximation-kde": {
						"id": "approximation-kde",
						"title": "Kernel Density Estimation",
						"category": "",
						"url": " /approximation/kde/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"approximation-pda": {
						"id": "approximation-pda",
						"title": "Probability Density Approximation",
						"category": "",
						"url": " /approximation/pda/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"approximation-ppda": {
						"id": "approximation-ppda",
						"title": "Parallel Probability Density Approximation",
						"category": "",
						"url": " /approximation/ppda/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"basics-descriptive": {
						"id": "basics-descriptive",
						"title": "Descriptive Statistics",
						"category": "",
						"url": " /basics/descriptive/",
						"content": "In most RT modelling work, researchers usually want to examine the manifested statistics. Often, these are the average response times (RTs) and accuracy rates. In the following, I used the LNR model LNR (Heathcote &amp; Love, 2012), as an example to illustrate a method to calculate these statistics efficiently. The user wishes to understand and apply LNR model on her his work can find useful information in the DMC tutorials (Heathcote et al., 2018). This particular LNR model presumes one stimulus (S) factor, and similar to the LBA model, it has a latent matching (M) factor. library(data.table); library(ggdmc) model &lt;- BuildModel( p.map = list(meanlog = \"M\", sdlog = \"M\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), responses = c(\"LEFT\", \"RIGHT\"), constants = c(st0 = 0), type = \"lnr\") The arbitrary chosen true parameters generate a reasonable RT distribution, which similar with typical choice RT data, giving approximately 25% errors (I will show you this later). p.vector &lt;- c(meanlog.true = -1, meanlog.false = 0, sdlog.true = 1, sdlog.false = 1, t0 = .2) simulate function takes the first option, model to generate data based on the provided model. ps option expects a true parameter vector that matches the setting in the model object, nsim option expects the number of trial per condition. dat &lt;- simulate(model, ps = p.vector, nsim = 1024) d &lt;- data.table(dat) ## S R RT ## 1: left LEFT 0.3821405 ## 2: left LEFT 0.7859101 ## 3: left LEFT 0.5237262 ## 4: left RIGHT 0.3932804 ## 5: left LEFT 0.6604592 ## --- ## 2044: right RIGHT 0.7342084 ## 2045: right RIGHT 1.3628130 ## 2046: right RIGHT 0.3343844 ## 2047: right RIGHT 0.4913930 ## 2048: right RIGHT 0.6119065 S is the stimulus factor R is the response type RT stores response time in second By using data.table function .N, I confirmed that each condition does has 1024 trials. d[, .N, .(S)] ## S N ## 1: left 1024 ## 2: right 1024 A similar syntax, with S and R factors, I printed out the information regarding the hit, correct rejection, false alarm and miss responses. d[, .N, .(S, R)] ## S R N ## assuming the left is signal and right is noise ## 1: left LEFT 791 ## hit ## 2: left RIGHT 233 ## miss ## 3: right RIGHT 786 ## correct rejection ## 4: right LEFT 238 ## false alarm I used a ifelse chain to calculate a C column to indicate correct (TRUE) and error (FALSE) responses. In real world data, there would be some responses missing or participants pressing wrong keys, so the last else is “NA” to catch these situations. d$C &lt;- ifelse(d$S == \"left\" &amp; d$R == \"LEFT\", TRUE, ifelse(d$S == \"right\" &amp; d$R == \"RIGHT\", TRUE, ifelse(d$S == \"left\" &amp; d$R == \"RIGHT\", FALSE, ifelse(d$S == \"right\" &amp; d$R == \"LEFT\", FALSE, NA)))) The data table now looks like below. ## S R RT C ## 1: left LEFT 0.3821405 TRUE ## 2: left LEFT 0.7859101 TRUE ## 3: left LEFT 0.5237262 TRUE ## 4: left RIGHT 0.3932804 FALSE ## 5: left LEFT 0.6604592 TRUE ## --- ## 2044: right RIGHT 0.7342084 TRUE ## 2045: right RIGHT 1.3628130 TRUE ## 2046: right RIGHT 0.3343844 TRUE ## 2047: right RIGHT 0.4913930 TRUE ## 2048: right RIGHT 0.6119065 TRUE This is one way to calculate average RTs with data.table. d[, .(MRT = round(mean(RT), 2)), .(C)] ## C MRT ## 1: TRUE 0.61 ## 2: FALSE 0.71 The syntax to calculate the response proportions, namely correct and error rates, are less straightforward, but possible. Firstly, I calculated the counts for hit, correct rejection, miss, and false alarm and store them in prop. Then I made up a new column, called NN, to store the total number of trial. Lastly, I divided the four conditions by the total number of trial. I also used a round to print only to the two decimal place below zero. These are almost 25% error rates, as promised. prop &lt;- d[, .N, .(S, R)] prop[, NN := sum(N), .(S)] prop[, acc := round(N NN, 2)] prop ## S R N NN acc ## 1: left LEFT 791 1024 0.77 ## 2: left RIGHT 233 1024 0.23 ## 3: right RIGHT 786 1024 0.77 ## 4: right LEFT 238 1024 0.23 Real-world Example In this section, I will demonstrate more data processing techniques, using the empirical data (Holmes, Trueblood &amp; Heathcote (2016). This data set can be downloaded from my OSF site. One raw data format often found is one subject per text or csv file (*.txt or *.csv). For example, the file, “S125.2014-04-23_6-22-36.txt”, stores the data from participant, S125. There are 47 of them. All are in the same format. In another tutorial, I will illustrate how to handle similar but not identical formatted data files. block trial target CO1 CO2 ST resp RT correct 1 1 R 50 -1 -1 1 1076 1 1 2 L 50 -1 -1 0 733 1 1 3 R 50 -1 -1 1 637 1 1 4 R 50 -1 -1 1 517 1 ... How to read large data sets efficiently I stored data files in a standard location of usual R packaging. The folder, named data unsurprisingly, immediately in a project folder. And the analysis scripts are stored in a folder, called R. I then used list.files function to store all file names in a object, called fn. ?list.files dp &lt;- \"data Holmes_etal_CogPsych_2016_Data\"; ## data path fn &lt;- list.files(dp, pattern = \"*.txt\") ## file name print(fn) ## [1] \"S125.2014-04-23_6-22-36.txt\" ## [2] \"S126.2014-04-23_6-25-38.txt\" ## [3] \"S127.2014-04-23_6-26-46.txt\" ## ... ## [45] \"S169.2014-05-07_6-16-49.txt\" ## [46] \"S170.2014-05-07_6-18-16.txt\" ## [47] \"S171.2014-05-07_6-38-56.txt\" Next I created a DTLapply function to pipe the text files one after another to the fread of data.table to quickly process them. function(fn, dp) { v &lt;- lapply(seq_along(fn), function(i) { s &lt;- strsplit(fn[i], split = \"[.]\")[[1]][1] d &lt;- data.table::fread(file.path(dp, fn[i])) S &lt;- d$target R &lt;- d$resp RTSec &lt;- d$RT 1e3 C &lt;- d$correct return(d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)]) }) return(data.table::rbindlist(v)) } x0 &lt;- DTLapply(fn, dp) seq_along function will convert fn, which store 47 file name strings to numerical indices, for the looping. seq_along(fn) [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 function(i) inside the lapply function is an anonymous function, namely a function used internally by the lappy function. Inside this anonymous function is basically a quicker R for loop. The first line in the anonymous function is to extract the label for a participant. For example, if I just processed the first text file, the strsplit function splits the string whenever it finds a dot symbol. Because strsplit returns an R list, I took the first element in the first list. print(fn[1]) ## [1] \"S125.2014-04-23_6-22-36.txt\" strsplit(fn[1], split = \"[.]\") ## [[1]] ## [1] \"S125\" \"2014-04-23_6-22-36\" \"txt\" s &lt;- strsplit(fn[i], split = \"[.]\")[[1]][1] print(s) ## [1] \"S125\" Next line uses the convenient function, file.path in R base to construct a file path to a particular file. For example, if I extract the first participant. dp &lt;- \"data Holmes_etal_CogPsych_2016_Data\"; ## data path file.path(dp) ## [1] \"data Holmes_etal_CogPsych_2016_Data\" file.path(dp, fn[1]) ## [1] \"data Holmes_etal_CogPsych_2016_Data S125.2014-04-23_6-22-36.txt\" The function, file.path returns the complete relative file path to the data file, which is then read by the fread function. Note I can easily use the relative path method, because the folder structure follows strictly R packaging structure. d &lt;- data.table::fread(file.path(dp, fn[1])) ## block trial target CO1 CO2 ST resp RT correct ## 1: 1 1 R 50 -1 -1 1 1076 1 ## 2: 1 2 L 50 -1 -1 0 733 1 ## 3: 1 3 R 50 -1 -1 1 637 1 ## 4: 1 4 R 50 -1 -1 1 517 1 ## 5: 1 5 L 50 -1 -1 0 476 1 ## --- ## 1276: 20 68 L 15 -1 -1 -1 -1 1 ## 1277: 20 69 L 15 -1 -1 0 1077 0 ## 1278: 20 70 LR 15 15 529 0 1463 1 ## 1279: 20 71 L 15 -1 -1 0 1895 0 ## 1280: 20 72 LR 15 15 529 0 1311 1 The following four lines were simply to temporarily save the columns, “target”, “resp”, “RT” (converted to second), and “correct” to four different R vectors, “S”, “R”, “RTSec”, and “C”. These operations just converted the original column naming to my factor naming convention. For example, target column indicates whether a stimulus was one of the four levels: right (stationary trial), left (stationary trial), left and right (switching trial) or right and left moving dot (switching trial), so I converted it to as stimulus, S, factor. Similarly, R factor is from the resp response column, RT column was converted to second, and correct column was converted to C. S &lt;- d$target R &lt;- d$resp RTSec &lt;- d$RT 1e3 C &lt;- d$correct These four R vectors were then grouped together as a R list. list(s, S, R, RTSec, C) ## [[1]] ## [1] \"S125\" ## [[2]] ## [1] \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\" \"L\" \"R\" \"L\" \"L\" ## [15] \"L\" \"R\" \"L\" \"R\" \"R\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" ## [29] \"L\" \"R\" \"L\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" ## ... ## [[3]] ## [1] 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 ## [25] 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 ## [49] 1 0 -1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 ## ... ## [[4]] ## [1] 1.076 0.733 0.637 0.517 0.476 0.419 0.493 0.486 0.685 0.581 ## [11] 0.460 0.462 0.666 0.557 0.446 0.438 0.549 0.358 0.486 0.516 ## [21] 0.484 0.589 0.679 0.743 0.550 0.646 0.516 0.486 0.588 0.463 ## [[5]] ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [38] 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 ## [75] 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 ## ... This list was directly inserted into the data.table, d, which created five new columns on top the original ones (i.e., block, trial, etc.). d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)] The result was then returned to the lapply function as its output. := is just the assignment symbol in data.table syntax. return(d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)]) Each participant file was looped through in a fastest possible way in R language and finally, each of them was glued together via the data.table function, rbindlist, which return as the output for my homemade DTLapply function. ## # A tibble: 60,160 x 13 ## block trial target CO1 CO2 ST resp RT correct s S R ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 1 R 50 -1 -1 1 1.08 1 S125 R 1 ## 2 1 2 L 50 -1 -1 0 0.733 1 S125 L 0 ## 3 1 3 R 50 -1 -1 1 0.637 1 S125 R 1 ## 4 1 4 R 50 -1 -1 1 0.517 1 S125 R 1 ## 5 1 5 L 50 -1 -1 0 0.476 1 S125 L 0 ## 6 1 6 R 50 -1 -1 1 0.419 1 S125 R 1 ## 7 1 7 R 50 -1 -1 1 0.493 1 S125 R 1 ## 8 1 8 R 50 -1 -1 1 0.486 1 S125 R 1 ## 9 1 9 L 50 -1 -1 0 0.685 1 S125 L 0 ## 10 1 10 L 50 -1 -1 0 0.581 1 S125 L 0 ## # ... with 60,150 more rows, and 1 more variable: C &lt;int&gt; How to trim off irregular participants It is not uncommon in a data set to have few participants who do not engage in performing a task or drop out in the middle of an experiment. With convincing evidence, we can exclude these participants. Here I demonstrated one way to conduct this operation. I used the match function, which has a symbol form, %in%. This is an R internal function, which uses efficient algorithm. ## Excluding 3 + 13 participants from model fitting, due to ## (1) a computer error and, ## (2) less than 70% accuracy on the stationary trials in 4-18 blocks. ## They are 126, 129, 130, 133, 134, 138, 139, 140, 143, 147, 148, 150, 155, ## 156, 161, 162 badsubjs &lt;- c(\"S126\", \"S129\", \"S130\", \"S133\", \"S134\", \"S138\", \"S139\", \"S140\", \"S143\", \"S147\", \"S148\", \"S150\", \"S155\", \"S156\", \"S161\", \"S162\") x1 &lt;- x0[ !(s %in% badsubjs) ] Reference Heathcote A., and Love J. (2012) Linear deterministic accumulator models of simple choice. frontiers in Psychology, 23. https: doi.org 10.3389 fpsyg.2012.00292. Holmes, W.R. et al, A new framework for modeling decisions about changing information: The Piecewise Linear Ballistic Accumulator model”, 2015, Cognitive Psychology 85, 1-29."
					}

					
				
			
		
			
				
					,
					

					"basics-mle": {
						"id": "basics-mle",
						"title": "Maximising Likelihoods",
						"category": "",
						"url": " /basics/mle/",
						"content": "This is a short note for one method to conduct maximum likelihood estimation (MLE) to fit the LBA model. In essence, the MLE is not a very difficult statistical technique, but there are some trivialities regarding to the cognitive model and its influence on the usage of optimiser that must be addressed. Otherwise, you would not recover the parameters in the LBA model (or cognitive models in general). In short, you must adjust either the objective function, the processing of data preparation, or how to propose parameters according to a specific cognitive model. For example, the non-decision time must not go below 0 second. If you do not (or cannot) add this constraint on the optimiser (e.g., the R function, optim), the resulting fit may not converge or the estimates (even converged) will be unreasonable, psychologically speaking. Below I use ggdmc to conduct a simulation study to demonstrate the point. Simulation study Firstly, as usual, I use the BuildModel function to set up a null model with only a stimulus factor (denoted S). That is, the model parameters do not associate with any factors. Next I arbitrarily set up a true parameter vector, p.vector, and request 100 trials per condition. My aim is to recover the true parameters. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\")), constants = c(st0 = 0, sd_v = 1), responses = c(\"r1\", \"r2\"), type = \"norm\") p.vector &lt;- c(A = .75, B = 1.25, t0 = .15, mean_v.true = 2.5, mean_v.false = 1.5) ntrial &lt;- 1e2 ## use the seed option to make sure I always replicate the result ## remove it, if you want to see the stochastic process. dat &lt;- simulate(model, nsim = ntrial, ps = p.vector, seed = 123) dmi &lt;- BuildDMI(dat, model) Description statistics As a good practice, we would mostly like to check basic descriptive statistics. Let’s see the RT distributions. Note there are two histograms (i.e., distributions). This is one of the specifics in the choice RT models. This is sometimes dubbed defective distributions, meaning multiple distributions jointly composing a complete model (integrated to 1). The likelihood_norm function in the ggdmc has considered this, so you will not see how the internal C++ codes handle this triviality. But if you use the bare-bones LBA density functions, say “ggdmc:::n1PDFfixedt0” (meaning node 1 probability density function), “ggdmc:::fptcdf” or “ggdmc:::fptpdf”, you need to handle the calculation of “defective distributions” accordingly. By the way, the top x axis in the above figure labels TRUE, representing correct responses and FALSE, representing error responses. It is not unusual to observe more correct responses than error responses, so the simulation produces realistic data. ## This is to create a column in the data frame to indicate ## correct and error responses. dmi$C &lt;- ifelse(dmi$S == \"s1\" &amp; dmi$R == \"r1\", TRUE, ifelse(dmi$S == \"s2\" &amp; dmi$R == \"r2\", TRUE, ifelse(dmi$S == \"s1\" &amp; dmi$R == \"r2\" ,FALSE, ifelse(dmi$S == \"s2\" &amp; dmi$R == \"r1\", FALSE, NA)))) prop.table(table(dmi$C)) ## FALSE == error responses (25.5%) ## TRUE == correct responses (74.5%) ## FALSE TRUE ## 0.255 0.745 ## The maximum (log) likelihoods den &lt;- likelihood_norm(p.vector, dmi) sum(log(den)) ## [1] -112.7387 Maximum likelihood estimation The following is the objective function. Note data must be a data model instance. This requirement is to use ggdmc internal to handle many trivialities, for instance, the defective distributions, experimental design, transforming parameter (), etc. If you use the bare-bones density functions, you must handle these trivialities. Also I use negative log likelihood. objective_fun &lt;- function(par, data) { den &lt;- likelihood_norm(par, data) return(-sum(log(den))) } init_par[3] &lt;- runif(1, 0, min(dmi$RT)) This line makes starting non-decision time not less than the minimal RT in the data. This is another psychological consideration. It may help. However, it does not guarantee the optimiser won’t propose a non-decision time less than minimal RT in the data. init_par &lt;- runif(5) init_par[3] &lt;- runif(1, 0, min(dmi$RT)) names(init_par) &lt;- c(\"A\", \"B\", \"t0\", \"mean_v.true\", \"mean_v.false\") res &lt;- nlminb(objective_fun, start = init_par, data = dmi, lower = 0) round(res$par, 2) ## remember to check res$convergence Below is a list of possible estimates. The last line show the true parameter vector for the convenience of comparison. The first column shows the numbers of trial per condition. At the size of 1e5, the recovered values almost equal to the true values. ## A B t0 mean_v.true mean_v.false ## 1e2 0.79 0.98 0.17 2.26 0.77 ## 1e2 0.86 1.74 0.04 2.80 1.82 ## 1e2 0.91 0.67 0.28 2.04 1.02 ## 1e2 0.72 1.36 0.14 2.74 1.60 ## 1e3 0.71 1.15 0.16 2.32 1.40 ## 1e3 0.61 1.63 0.08 2.70 1.76 ## 1e4 0.71 1.28 0.15 2.51 1.50 ## 1e5 0.75 1.24 0.15 2.49 1.49 ## true 0.75 1.25 0.15 2.50 1.50 Instead of using the optim function, I opt to nlminb function. This is again a model specific consideration. In the LBA model, A, B, and t0 must not be less than 0, so it will help if we can impose this constraint. Both optim and nlminb offer an argument, lower, to constraint the parameter proposals. However, if you impose the lower constraint, optim allows only (?) the optimisation method, “L-BFGS-B”, which does not handle well infinite. Unfortunately, in fitting the LBA model, it is likely some parameter proposals result in infinite log-likelihoods. Bonus A better way to initialise a parameter proposal is to use prior distributions. rprior in ggdmc allows you to easily do this. p.prior &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"beta\", \"tnorm\", \"tnorm\"), p1 = c(A = 1, B = 1, t0 = 1, mean_v.true = 1, mean_v.false = 1), p2 = c(1, 1, 1, 1, 1), lower = c(rep(0, 3), rep(NA, 2)), upper = c(rep(NA, 2), 1, rep(NA, 2))) init_par &lt;- rprior(p.prior) ## A B t0 mean_v.true mean_v.false ## 0.40 0.65 0.24 0.89 -0.26"
					}

					
				
			
		
			
				
					,
					

					"basics-model-array": {
						"id": "basics-model-array",
						"title": "Model Array",
						"category": "",
						"url": " /basics/model_array/",
						"content": "The method ggdmc adapts different factorial designs is to use Boolean model matrices, which associate experimental conditions with latent variables model (free) parameters. The model parameters are often designed to account for cognitive operations that cannot be directly observed. Three examples are the rate of the degradation of memory strength, the rate of (sensory) evidence accumulation, and the response threshold. Take regression models for example. One might be interested in examining intercepts and slopes, the two regression model parameters by themselves usually do not carry psychological meanings. Of course, one can construct a framework to harness the (regression) model parameters. For example, in traditional visual search studies, mean response times (MRTs) are often associated with the display sizes and the slopes of the MRT-Display size function were conveniently interpreted as search efficiency (Treisman &amp; Gelade, 1980). This was useful strategy as a staring point, but needs further refinement to get more insights (e.g., to understand speed-accuracy trade-off issue, serial vs. parallel processing etc.). It is therefore and often needed to refine the basic regression model to further accommodate many intricate cognitive constructs. ggdmc hard-wires, the diffusion decision and the linear ballistic models and applies the method of Boolean matrices to serve the purpose of adapting factorial designs and that of accounting for latent variables of RT models. The first step in ggdmc is to set up a 3-D model array. Build Models BuildModel creates a model array, which composes of many model matrices. Each of them represents a response. The content of a model matrix indicates the correspondence of parameters and design cells. For example, if a data set has a two-level stimulus factor, affecting the drift rate (as in DDM), a model matrix will have two drift rate parameters, say, v.d1 and v.d2 (d stands for difficulty). One could understand this idea of correspondence between an experimental factor and its parameter mapping by examining the following example. Example 1 In this example, I used the LBA model (Brown and Heathcote, 2008) to illustrate, fitting data from a single participant. The LBA’s B parameter depends only on response (R). The mean and the standard deviation of the drift rates depends on M (matching) factor. The experimental design has one two-level stimulus factor (S). The following model presumes the S factor has no effect on any model parameter. The accuracy is determined by S and R. The M factor is a specific latent factor just for the LBA model. model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") p.map means parameter map. match.map matches the stimulus type to the response type to determine if a response is correct or error. factors means experimental factors, constants specifies which model parameter to fix as constant values. This is to enforce model assumptions. responses indicates response types, by specifying character strings or numbers. Lastly, type specifies the model types, such as the diffusion decision model (rd) or the LBA (norm). For illustration purpose, I simulated some realistic response time data. I made up a true parameter vector. This is usually unknown and estimated from data. p.vector &lt;- c(A = .75, B.r1 = .25, B.r2 = .15, t0 = .2, mean_v.true = 2.5, mean_v.false = 1.5, sd_v.true = 0.5) print will show the model array together with its attributes that have been added into in the BuildModel step. print(model) ## r1 ## A B.r1 B.r2 t0 mean_v.true mean_v.false sd_v.true sd_v.false st0 ## s1.r1 TRUE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE ## s2.r1 TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## s1.r2 TRUE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE ## s2.r2 TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## r2 ## A B.r1 B.r2 t0 mean_v.true mean_v.false sd_v.true sd_v.false st0 ## s1.r1 TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## s2.r1 TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## s1.r2 TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## s2.r2 TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## model has the following attributes: ## [1] \"dim\" \"dimnames\" \"all.par\" \"p.vector\" \"pca\" \"par.names\" ## [7] \"type\" \"factors\" \"responses\" \"constants\" \"posdrift\" \"n1.order\" ## [13] \"match.cell\" \"match.map\" \"class\" print, when supplied with a true parameter vector, will show how the factorial design is assigned to model parameters. Understanding the assigning process is an advanced topic. I will return to it at a different tutorial. Note that I, using Brown and Heathcote’s (2008) convention, differentiate the lowercase b and uppercase B in the LBA model. The former means the threshold parameter, and the latter is the travel distance parameter. The LBA model assumes b = A + B. print(model, p.vector) ## \"s1.r1\" ## A b t0 mean_v sd_v st0 ## 0.75 1.0 0.2 2.5 0.5 0 ## 0.75 0.9 0.2 1.5 1.0 0 ## \"s2.r1\" ## A b t0 mean_v sd_v st0 ## 0.75 1.0 0.2 1.5 1.0 0 ## 0.75 0.9 0.2 2.5 0.5 0 ## \"s1.r2\" ## A b t0 mean_v sd_v st0 ## 0.75 0.9 0.2 1.5 1.0 0 ## 0.75 1.0 0.2 2.5 0.5 0 ## \"s2.r2\" ## A b t0 mean_v sd_v st0 ## 0.75 0.9 0.2 2.5 0.5 0 ## 0.75 1.0 0.2 1.5 1.0 0"
					}

					
				
			
		
			
				
					,
					

					"basics-simulation": {
						"id": "basics-simulation",
						"title": "Simulation",
						"category": "",
						"url": " /basics/simulation/",
						"content": "This lesson has two sections. First demonstrates a method to simulate one-participant data. The function, simulate, in the ggdmc package creates a data frame based on the parameter vector and the model (both are defined by a user) with nsim observations for each row in model. ps is the true parameter vector. Second section shows a method to conduct a process model. Specifically, the section conducts a simulation experiment to describe the British tea example on p 37 in Maxwell &amp; Delaney (2004). See Maxwell and Delaney (2004) for an analytic method to calculate the same probabilities. Here I directly model the Britich tea example, approximating the same probabilities. The analytic method is just to use a binominal distribution and the idea of combinations and permutations. One-participant simulation This line define one S (stimulus) factor with two levels. So this model defines one two experimental conditions. factors = list(S = c(“s1”, “s2”)), Below are the R codes for defining a model and for simulating data from the model. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), ## one factor with two levels, so only constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") p.vector &lt;- c(A = .75, B.r1 = .25, B.r2 = .15, t0 = .2, mean_v.true = 2.5, mean_v.false = 1.5, sd_v.true = 0.5) This just is to simulate only one observation per condition to check the function. set.seed(123) ## Set seed to get the same simulation dat &lt;- simulate(model, nsim = 1, ps = p.vector) ## S R RT ## 1 s1 r1 0.3327392 ## 2 s2 r1 0.3797985 The following simulates 500 observations per condition. So in total, there are 1000 observations. ntrial &lt;- 5e2 ## number of trials per condition dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dplyr::tbl_df(dat) ## A tibble: 1,000 x 3 ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r2 0.533 ## 2 s1 r2 0.494 ## 3 s1 r1 0.497 ## 4 s1 r2 0.310 ## 5 s1 r1 0.462 ## 6 s1 r2 0.345 ## 7 s1 r2 0.430 ## 8 s1 r1 0.384 ## 9 s1 r2 0.310 ## 10 s1 r1 0.302 # # ... with 990 more rows Note that model and data are in fact two separate objects. To fit data with certain models, we need to bind them together with BuildDMI. This is useful to facilitate model comparison. That is, a data set can bind with many different models, so we can compare them to see which model may fit the data better so perhaps provide a better account. I used a term, data-model instance (dmi), coined by Matthew Gretton. dmi &lt;- BuildDMI(dat, model) We can the codes introduced in the “Descriptive Statistics” to check the correct 10%, 50%, 90% quantile RTs and accuracy, separately, for each level of the stimulus factor. First I convert the dmi data frame to a data table and then create a new accuracy (logical) column, C. require(data.table) d &lt;- data.table(dmi) d$C &lt;- ifelse(d$S == \"s1\" &amp; d$R == \"r1\", TRUE, ifelse(d$S == \"s2\" &amp; d$R == \"r2\", TRUE, ifelse(d$S == \"s1\" &amp; d$R == \"r2\", FALSE, ifelse(d$S == \"s2\" &amp; d$R == \"r1\", FALSE, NA)))) d[, .(q1 = round(quantile(RT, .1), 2), q5 = round(quantile(RT, .5), 2), q9 = round(quantile(RT, .9), 2)), .(C, S)] ## C S q1 q5 q9 ## 1: TRUE s1 0.32 0.42 0.56 ## 2: TRUE s2 0.28 0.39 0.52 ## 3: FALSE s1 0.27 0.37 0.50 ## 4: FALSE s2 0.32 0.39 0.54 pro &lt;- d[, .N, .(C, S)] pro[, NN := sum(N), .(S)] pro[, value := N NN] cp &lt;- pro[C == TRUE] ## correct percentage ## C S N NN value ## 1: TRUE s1 333 500 0.666 ## 2: TRUE s2 391 500 0.782 ep &lt;- pro[C == FALSE] ## error percentage ## C S N NN value ## 1: FALSE s1 167 500 0.334 ## 2: FALSE s2 109 500 0.218 Plot the RT distributions require(ggplot2) bw &lt;- .01 ## 10 ms binwidth p0 &lt;- ggplot(d, aes(RT)) + geom_histogram(binwidth = bw, fill = \"white\", colour = \"black\") + facet_grid(.~C) + theme_bw(base_size = 18) print(p0) British tea example This section shows how we may test an hypothetical question directly via a simulation. Quoted from Maxwell and Delaney (p. 37, 2004) “A lady declares that by tasting a cup of tea made with milk, she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. (Fisher, 1935 1971, p. 11)” This is essential a binominal decision making. That is, the decision maker (“the lady”) in question will be presented one cup of tea after another and then her task is to decide if the cup is made by milk or tea is added first. This following function, British.tea implements a process model to describe the above “British tea example”. That is, it conducts a simulation experiment of presenting 8 (i.e., n) cups of tea to a participant. The n equals 8 is decided arbitrarily here. One additional information (i.e., assumption) is that the participant is told half of the cups are milk first and tea and vice versa. So when simulating the chance only scenario, we need also to take this into consideration. That is, after making a decision for a cup (either MT or TM), the (chance) probability state should adjust accordingly. ##' British tea example ##' ##' The function runs a simulation study to test the British tea example ##' ##' @param the number of observation (cups of tea) ##' @param correct correct sequence: First four cups are tea and milk ## (TM = 1), the next four cups are milk and then tea (MT = 0). ##' @param verbose print more information ##' ##' @export British.tea &lt;- function(n = 8, correct = c(1,1,1,1, 0,0,0,0), verbose = TRUE) { MT &lt;- n 2 ## 0 indicates milk and then tea (MT) TM &lt;- n 2 ## 1 indicates tea and then milk (TM) ## Create three containers ## 1. x0 is a \"n x 2\" matrix to store the evolution of chance probabilities ## 2. res is a n-element numeric vector ## 3. acc is a n logical vector; default value is FALSE x0 &lt;- matrix(numeric(n*2), ncol = 2) res &lt;- numeric(n) acc &lt;- rep(FALSE, n) ## Begin the experiment, presenting one cup after another for (i in 1:n) { if (verbose) cat(\"Cup\", i, \"in total\", sum(MT, TM), \" cup(s)\\n\") ## store the chance probabilities of MT and TM in probs probs &lt;- c(MT (MT + TM), TM (MT + TM)) if (verbose) cat(\"Chances probabilities of (MT, TM): \", probs, \"\\n\") x0[i, ] &lt;- probs decision &lt;- sample(c(0, 1), 1, prob = probs); if (decision == 0) { if (verbose) cat(\"This cup is made by adding milk first\\n\") MT &lt;- MT - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else if (decision == 1) { if (verbose) cat(\"This cup is made by adding tea first\\n\") TM &lt;- TM - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else cat(\"Unexpected situation\\n\") if (verbose) cat(\"Current state\", i, \": \", c(MT, TM), \"\\n\\n\") } if (verbose) cat(\"Done\\n\") return(list(x0, res, correct, acc)) } The simulation starts from the for loop, for (i in 1:n) {…}, which represents presenting a cup of tea after another until the last nth cup. Before the participant make a decision regarding each cup of tea, the chance probabilities of the two possible outcomes are stored in x0 variable. probs &lt;- c(probMT, probTM) x0[i, ] &lt;- probs And then the sample function acts as a chance mechanism to simulate the participant’s (chance) decision making process. decision &lt;- sample(c(0, 1), 1, replace = TRUE, prob = probs); The function randomly choose two numbers, c(0, 1), with the probabilities, probs to for the first and second number. ## Begin the experiment, presenting one cup after another for (i in 1:n) { if (verbose) cat(\"Cup\", i, \"in total\", sum(MT, TM), \" cup(s)\\n\") probMT &lt;- MT (MT + TM) ## chance probability of MT 0 probTM &lt;- TM (MT + TM) ## chance probability of TM 1 probs &lt;- c(probMT, probTM) if (verbose) cat(\"Chances probabilities of (MT, TM): \", probs, \"\\n\") x0[i, ] &lt;- probs decision &lt;- sample(c(0, 1), 1, prob = probs); if (decision == 0) { if (verbose) cat(\"This cup is made by adding milk first\\n\") MT &lt;- MT - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else if (decision == 1) { if (verbose) cat(\"This cup is made by adding tea first\\n\") TM &lt;- TM - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else cat(\"Unexpected situation\\n\") if (verbose) cat(\"Current state\", i, \": \", c(MT, TM), \"\\n\\n\") } Conduct one experiment and print information ncup &lt;- 8 cor &lt;- c(rep(1, 4), rep(0, 4)); res &lt;- British.tea(ncup, cor, TRUE) Cup 1 in total 8 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 1 : 4 3 Cup 2 in total 7 cup(s). Chances probabilities of (MT, TM): 0.5714286 0.4285714 This cup is made by adding milk first Current state 2 : 3 3 Cup 3 in total 6 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 3 : 3 2 Cup 4 in total 5 cup(s). Chances probabilities of (MT, TM): 0.6 0.4 This cup is made by adding milk first Current state 4 : 2 2 Cup 5 in total 4 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding milk first Current state 5 : 1 2 Cup 6 in total 3 cup(s). Chances probabilities of (MT, TM): 0.3333333 0.6666667 This cup is made by adding tea first Current state 6 : 1 1 Cup 7 in total 2 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 7 : 1 0 Cup 8 in total 1 cup(s). Chances probabilities of (MT, TM): 1 0 This cup is made by adding milk first Current state 8 : 0 0 Done Now I replicate the experiments separately for 512, 4096, 32768, 262144, and 2097152 times and store each result in a list, called exp. n &lt;- 8^(3:7); exp &lt;- vector(\"list\", length(n)) ## Use parallel package to conduct experiments ## 100.636 s library(parallel) cl &lt;- makeCluster(detectCores()) clusterExport(cl, c(\"British.tea\", \"ncup\", \"cor\")) system.time( for (i in 1:length(n)) { exp[[i]] &lt;- parSapply(cl, 1:n[i], function(i, ...) {British.tea(ncup, cor, FALSE)} ) } ) stopCluster(cl) ## Without using parallel ## for(i in 1:length(n)) { ## exp[[i]] &lt;- replicate(n[i], British.tea(ncup, cor, FALSE)) ## } res3 &lt;- numeric(length(n)); res3 ## to store the result when 6 corrects res4 &lt;- numeric(length(n)); res4 ## to store the result when 8 corrects ## Collect results for(i in 1:length(n)) { c3 &lt;- 0 c4 &lt;- 0 for(j in 1:n[i]) { ## Calculate exactly 4 corrects if(all(exp[[i]][,j][[4]])) c4 &lt;- c4 + 1 if(sum(exp[[i]][,j][[4]]) == 6) c3 &lt;- c3 + 1 } res3[i] &lt;- c3 n[i] res4[i] &lt;- c4 n[i] } round(res3, 4) ## [1] 0.2578 0.2324 0.2306 0.2288 0.2286 round(res4, 4) ## [1] 0.0137 0.0137 0.0140 0.0141 0.0143 require(ggplot2); require(data.table) ## Plot the result ## (How to add differernt horizontal lines on each facet) DT &lt;- data.table(x= rep(n, 2), y = c(res3, res4), gp = rep(c(\"6\", \"8\"), each = 5), ref = rep(c(16 70, 1 70), each = 5)) ## Dashlines show theoretically probabilities p0 &lt;- ggplot(DT, aes(x, y)) + geom_point(size = 3) + geom_hline(aes(yintercept = ref), linetype = \"dashed\") + ## scale_x_log10(name = \"N\") + xlab(\"N\") + ylab(\"Probability\") + facet_grid(gp~., scales = \"free\") + theme_bw(base_size = 22) print(p0)"
					}

					
				
			
		
			
				
					,
					

					"basics-summary": {
						"id": "basics-summary",
						"title": "Summary Statistics",
						"category": "",
						"url": " /basics/summary/",
						"content": "In analyzing response time data with two choices, researchers would usually examine average response times (RTs) and response proportions. Depending on the model a researcher wishes to presume, the response proportions can simply be correct and error rates, or, if using SDT model, hits, correct rejections, false alarms and misses. Here I used Pleskac, Cesario, and Johnson’s (2017) data in the first-person shooter task (FPST; Correll et al., 2002) to illustrate one method to calculate average RTs and response proportions across participants. Firstly, I use fread function to load the data file, which is in csv format. The data set provides clear and good column names. That is, the column names have informed the coding method. I simply just followed column names to code the factor levels and later checked against the data in the paper. Of course, I had also checked against the figures of behaviour analyses in the paper to make sure I did correctly identify the dependent and independent variables, which are listed in the following. S: stimulus factor, gun vs. non-gun objects. BC: blurry or clear object CT: context, a safe or dangerous neighborhood RACE: race, a black or white target R: response factor, shoot or not to shoot RT: response times s: subject participant nominal labels library(data.table); study3 &lt;- fread(\"data race Study3 original Study3TrialData.csv\") study3$S &lt;- factor(ifelse(study3$Object0NG1G == 0, \"non\", \"gun\")) study3$BC &lt;- factor(ifelse(study3$Blurry0Clear1Blur == 0, \"clear\", \"blur\")) study3$CT &lt;- factor(ifelse(study3$Context1Safe2Danger == 0, \"safe\", \"danger\")) study3$RACE &lt;- factor(ifelse(study3$Race012B == 0, \"white\", \"black\")) study3$R &lt;- factor(ifelse(study3$Resp0NS1Sh == 0, \"not\", \"shoot\")) study3$RT &lt;- study3$RT 1e3 study3$s &lt;- factor(study3$Subject) factor is a R function converting variables, numeric or character, to categorical (i.e., nominal) variable. After reorganizing the columns, I removed the replicated columns by assigning them as NULL. This is a data.table specific syntax. study3[, c(\"Subject\", \"NewSubject\", \"conditionRaceDangerBlurbject\", \"conditionRaceDangerBlur\", \"Object0NG1G\", \"Blurry0Clear1Blur\", \"Context1Safe2Danger\", \"Race012B\", \"Resp0NS1Sh\", \"DiffusionRT\") := NULL] There are NaN response times in this data set. One method is to replace them with random RTs drawn from uniform distribution, with the range of valid RTs. This was achieved by using the data.table internal function .I. I firstly found the (row) index of these NaN RTs, and then replaced them. Of course, we can simply just remove them. ## save organized data to a temporary object, so I can roll back. dtmp &lt;- data.table(study3) minmax &lt;- range(study3$RT, na.rm = TRUE); minmax idx &lt;- dtmp[, .I[is.nan(RT)]]; idx dtmp[idx, RT := runif(1, minmax[1], minmax[2])] d &lt;- dtmp ## scoring a correctness column d$C &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", FALSE, NA)))) Now the data table looks like: dplyr::tbl_df(d) ## # A tibble: 12,033 x 8 ## RT S BC CT RACE R s C ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; ## 1 0.753 gun blur safe black shoot 11 TRUE ## 2 0.851 non blur safe white not 11 TRUE ## 3 0.742 gun clear safe black shoot 11 TRUE ## 4 0.636 non clear safe white not 11 TRUE ## 5 0.644 gun blur safe black not 11 FALSE ## 6 0.625 non clear safe black shoot 11 FALSE ## 7 0.889 non clear safe white not 11 TRUE ## 8 0.597 gun blur safe black shoot 11 TRUE ## 9 0.724 gun clear safe white shoot 11 TRUE ## 10 0.656 non blur safe white not 11 TRUE ## # ... with 12,023 more rows Censoring RT data Censoring outliers is a difficult task (Ratcliff, 1993). Here I illustrated one way to do it via Heathcote’s rc, a collection of very useful R functions and my summarise, also a collection of useful R functions. First, I used R’s source function to load this large collection of R functions. source(\"~ rc data.analysis.R\") source(\"~ rc utils.R\") source(\"~ functions summarise.R\") ## Scoring ------------ se3 &lt;- score.rc(data.frame(d), S = \"s\", R = \"R\", RT = \"RT\", SC = \"C\", F = c(\"BC\", \"CT\", \"RACE\", \"S\")) ## Spreading 11851 of 12033 RTs that are ties given preceision 0.001 . ## 497 have ties out of 679 unique values ## ## Added the following manifest design ## S RACE CT BC R rcell ## 1 gun black danger blur not 1 ## 2 gun black danger blur shoot 1 ## 3 gun black danger clear not 2 ## ... ## 30 non white safe blur shoot 15 ## 31 non white safe clear not 16 ## 32 non white safe clear shoot 16 score.rc function takes first argument data.frame, which is the data as seen previously. Because I stored it as data.table, I needed to convert it back to data.frame. Just a note. Although data.table may accommodate many functions operating in data.frame, there are some operations in rc functions, which cannot work in data.table. Note the second argument, uppercase S, which takes the subject column, instead of the column of stimulus factor. The R and RT arguments take response column and the response time column. SC takes the column of score correctness, which is purely my guess. I cannot be sure why it is called SC. The last useful argument is F, which takes user-defined factors, including the stimulus factor. score.rc detects the identical (ties) RTs and spread them into finer scale. For example, in this data set, there are 31 trials with 60y ms. table(d$RT) ## 0.01 0.015 0.019 0.025 ## 1 1 1 1 ## 0.027 0.03 0.035 0.036 ## 1 1 1 1 ## ... ## 0.386 0.387 0.388 0.389 ## 7 3 1 3 ## 0.39 0.391 0.392 0.393 ## 1 4 3 3 ## 0.394 0.395 0.396 0.397 ## 2 6 7 3 ## ... ## 0.606 0.607 0.608 0.609 ## 50 31 40 39 ## 0.61 0.611 0.612 0.613 ## 42 49 31 41 ## 0.614 0.615 0.616 0.617 ## 49 55 42 39 ## ... If I printed them all out, the data set after scoring spreads these RT to a se3[se3$RT &gt;= .607 &amp; se3$RT &lt; .608,] cell rcell s BC CT RACE S C R RT 539 31 16 19 clear safe white non TRUE not 0.6070000 838 31 16 24 clear safe white non TRUE not 0.6075488 1909 26 13 39 blur danger white non FALSE shoot 0.6070313 2108 6 3 44 blur safe black gun TRUE shoot 0.6072812 2321 4 2 50 clear danger black gun TRUE shoot 0.6079634 2354 19 10 50 clear danger black non TRUE not 0.6071250 2939 15 8 62 clear safe white gun FALSE not 0.6072188 3083 32 16 62 clear safe white non FALSE shoot 0.6077683 3319 12 6 72 clear danger white gun TRUE shoot 0.6075244 3762 19 10 82 clear danger black non TRUE not 0.6079146 4802 23 12 120 clear safe black non TRUE not 0.6076951 5211 17 9 129 blur danger black non TRUE not 0.6075976 5391 19 10 129 clear danger black non TRUE not 0.6074063 6095 1 1 184 blur danger black gun FALSE not 0.6073125 7057 10 5 201 blur danger white gun TRUE shoot 0.6077195 7201 17 9 201 blur danger black non TRUE not 0.6078659 7288 6 3 214 blur safe black gun TRUE shoot 0.6073438 7345 25 13 214 blur danger white non TRUE not 0.6071875 7747 23 12 218 clear safe black non TRUE not 0.6077439 8259 8 4 231 clear safe black gun TRUE shoot 0.6076707 8305 19 10 231 clear danger black non TRUE not 0.6079390 8671 12 6 235 clear danger white gun TRUE shoot 0.6071563 8923 19 10 247 clear danger black non TRUE not 0.6073750 9002 31 16 247 clear safe white non TRUE not 0.6074375 9045 31 16 247 clear safe white non TRUE not 0.6070625 9509 31 16 286 clear safe white non TRUE not 0.6076220 9551 31 16 286 clear safe white non TRUE not 0.6078415 9692 6 3 286 blur safe black gun TRUE shoot 0.6075732 9903 27 14 288 clear danger white non TRUE not 0.6079878 10432 25 13 307 blur danger white non TRUE not 0.6077927 10534 10 5 308 blur danger white gun TRUE shoot 0.6078902 10666 21 11 308 blur safe black non TRUE not 0.6074688 10979 16 8 325 clear safe white gun TRUE shoot 0.6078171 11201 9 5 326 blur danger white gun FALSE not 0.6070937 11642 19 10 344 clear danger black non TRUE not 0.6072500 11866 25 13 348 blur danger white non TRUE not 0.6076463 The original data set is to the millisecond scale. d[RT == .607] RT S BC CT RACE R s C 1: 0.607 gun blur danger black not 11 FALSE 2: 0.607 non blur danger black not 19 TRUE 3: 0.607 non clear safe white not 19 TRUE 4: 0.607 gun blur safe black shoot 24 TRUE 5: 0.607 non clear danger white not 28 TRUE 6: 0.607 non clear safe white not 37 TRUE 7: 0.607 non blur danger white shoot 39 FALSE 8: 0.607 non blur danger black not 44 TRUE 9: 0.607 gun blur safe black shoot 44 TRUE 10: 0.607 non clear danger black not 50 TRUE 11: 0.607 gun clear danger white shoot 50 TRUE 12: 0.607 gun clear safe white not 62 FALSE 13: 0.607 non clear danger black not 129 TRUE 14: 0.607 gun blur danger black not 184 FALSE 15: 0.607 non clear safe white not 184 TRUE 16: 0.607 non blur danger black not 184 TRUE 17: 0.607 gun blur safe black shoot 214 TRUE 18: 0.607 non blur danger white not 214 TRUE 19: 0.607 non clear safe white not 218 TRUE 20: 0.607 gun clear danger white shoot 235 TRUE 21: 0.607 non clear danger black not 247 TRUE 22: 0.607 non clear safe white not 247 TRUE 23: 0.607 non clear safe white not 247 TRUE 24: 0.607 gun clear danger white shoot 288 TRUE 25: 0.607 non blur safe black not 308 TRUE 26: 0.607 non clear safe white not 325 TRUE 27: 0.607 gun blur safe white shoot 326 TRUE 28: 0.607 gun blur danger black shoot 326 TRUE 29: 0.607 gun blur danger white not 326 FALSE 30: 0.607 non blur danger black not 344 TRUE 31: 0.607 non clear danger black not 344 TRUE RT S BC CT RACE R s C The scored data set, se3, will also attach two new columns, cell and rcell, indicating the experimental design. In this example, it has 32 cell, so cell is from 1 to 32 and rcell is from 1 to 16, because this is a two-choice experiment, response, shoot and not to shoot in cell 1 and cell 2, belong to the same experimental design, but with different response types. ## 1 gun black danger blur not 1 ## 2 gun black danger blur shoot 1 ## 3 gun black danger clear not 2 ## 4 gun black danger clear shoot 2 A usual practice is to take 3 times the standard deviation, respectively in each participants. This can be achieved via tapply function. If the data set is large, one can use data.table to achieve the same aim, which I will demonstrate in a later tutorial. sd3 &lt;- tapply(se3$RT, se3$s, mean) + tapply(se3$RT, se3$s, sd) * 3; A second useful function in rc collection is the make.rc, which does the censoring work. It takes a first argument of the scored data set, from score.rc and a second argument, correct.name, indicating the character string for the correctness column, and the last two arguments, for the lower and upper bounds of the censoring. me3 &lt;- make.rc(se3, correct.name = \"C\", minrt = .2, maxrt = sd3) How to average across trials mv: measurement dependent variable gvs: grouping variables wvs: within variables acc0 &lt;- summarySE(d, mv = \"error\", gvs = c(\"s\", \"BC\", \"CT\", \"RACE\", \"S\")) mrt0 &lt;- summarySE(d[C == TRUE], mv = \"RT\", gvs = c(\"s\", \"BC\", \"CT\", \"RACE\", \"S\")) ## Within se average across subjects for pc and nt figA &lt;- summarySEwithin(acc0, wvs = c(\"BC\",\"CT\", \"RACE\", \"S\"), mv = \"error\") figB &lt;- summarySEwithin(mrt0, wvs = c(\"BC\",\"CT\", \"RACE\", \"S\"), mv = \"RT\") names(figA) &lt;- c(\"BC\", \"CT\", \"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") names(figB) &lt;- c(\"BC\", \"CT\", \"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") Reference Ratcliff, R. (1993). Methods for dealing with reaction time outliers. Psychological bulletin, 114(3), 510."
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-diagnosis": {
						"id": "bayes-basics-diagnosis",
						"title": "Checking Fitted Models",
						"category": "",
						"url": " /bayes-basics/diagnosis/",
						"content": "This page (temporarily) documents four different plots for checking fitted models. The example simulates a data set from the regression normal model. rm(list = ls()) model &lt;- BuildModel( p.map = list(a = \"1\", b = \"1\", tau = \"1\"), match.map = NULL, regressors= c(8, 15, 22, 29, 36), factors = list(S = c(\"x1\")), responses = \"r1\", constants = NULL, type = \"glm\") p.vector &lt;- c(a = 242.7, b = 6.185, tau = .01) ntrial &lt;- 1000 dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dmi &lt;- BuildDMI(dat, model) npar &lt;- length(GetPNames(model)) start &lt;- BuildPrior( dists = c(\"tnorm2\", \"tnorm2\", \"gamma\"), p1 = c(a = 240, b = 6, tau = .01), p2 = c(a = 1e-6, b = 1e-6, tau = .1), lower = c(NA, NA, NA), upper = rep(NA, npar)) p.prior &lt;- BuildPrior( dists = c(\"tnorm2\", \"tnorm2\", \"gamma\"), p1 = c(a = 200, b = 0, tau = .1), p2 = c(a = 1e-6, b = 1e-6, tau = .1), lower = c(NA, NA, NA), upper = rep(NA, npar)) ## Sampling ----------- fit0 &lt;- Start_glm(5e2, dmi, start, p.prior, thin = 8) fit &lt;- run(fit0, pm0 = .05) fit &lt;- run(RestartSamples(5e2, fit, thin = 8)) Trace and density plots The first two are trace and density plots. p0 &lt;- plot(fit) p1 &lt;- plot(fit, pll = FALSE, den = TRUE) Autocorrelation plots The third is the autocorrelation plot. This first figure plots all chains and the second randomly selects a subset of three chains to construct the figure. The latter function is useful when fitting a model with many parameters and the model fit uses a large number of chains. By default, the “autocor” calculates to 50 lags. p2 &lt;- autocor(fit) p3 &lt;- autocor(fit, nsubchain = 3) Correlation matrix The fourth is the plot of correlation matrix. This plot is useful to check (post hoc) the association among model parameters. This needs to use the ggpairs function in GGally package. pairs.model &lt;- function(x, start = 1, end = NA, ...) { if (x$n.chains == 1) stop (\"MCMC needs multiple chains to check convergence\") if (is.null(x$theta)) stop(\"Use hyper mcmc_list\") if (is.na(end)) end &lt;- x$nmc if (end &lt;= start) stop(\"End must be greater than start\") d &lt;- ConvertChains(x, start, end, FALSE) D_wide &lt;- data.table::dcast.data.table(d, Iteration + Chain ~ Parameter, value.var = \"value\") bracket_names &lt;- names(D_wide) par_cols &lt;- !(bracket_names %in% c(\"Iteration\", \"Chain\")) p0 &lt;- GGally::ggpairs(D_wide, columnLabels = bracket_names[par_cols], columns = which(par_cols), ...) print(p0) return(invisible(p0)) } p5 &lt;- pairs.model(fit) p6 &lt;- pairs.model(fit, lower = list(continuous = \"density\")) The additional option in p6 is to choose to plot density contour. lower = list(continuous = “density”)"
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-likelihood": {
						"id": "bayes-basics-likelihood",
						"title": "Model Likelihood",
						"category": "",
						"url": " /bayes-basics/likelihood/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-posterior": {
						"id": "bayes-basics-posterior",
						"title": "Posterior Distribution",
						"category": "",
						"url": " /bayes-basics/posterior/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-prior": {
						"id": "bayes-basics-prior",
						"title": "Prior Distribution",
						"category": "",
						"url": " /bayes-basics/prior/",
						"content": "In Bayesian computation, a prior distribution refers to a similar, but slightly different idea from the original Bayes’ theorem. I used the diffusion decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea. The full DDM has eight parameters. In ggdmc (as well as DMC) syntax, they are defined as following: p.map = list(a = “1”, v = “1”, z = “1”, d = “1”, sz = “1”, sv = “1”, t0 = “1”, st0 = “1”), a: the boundary separation v: the mean of the drift rate z: the mean of the starting point of the diffusion relative to threshold separation d: differences in the non-decisional component between upper and lower threshold sz: the width of the support of the distribution of zr sv: the standard deviation of the drift rate t0: the mean of the non-decisional component of the response time st0: the width of the support of the distribution of t0 The question is how do we determine the values for these parameters. This is where prior distribution comes in. We presume there are eight distributions jointly determine the DDM prior distribution and these eight distributions are where we draw the realized parameter values. This way, the parameter values are said stochastic, rather than deterministic. In other words, the value, for instance boundary separation, a, changes every time we consult its prior distribution. It is decided probabilistically by its prior distribution. Below I list the full command, BuildModel, for setting up a DDM model. ## Use verbose option to suppress printing p.vector ## This is a DDM model with no manipulation factor model &lt;- BuildModel( p.map = list(a = \"1\", v = \"1\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), constants = c(st0 = 0, d = 0), responses = c(\"LEFT\", \"RIGHT\"), type = \"rd\", verbose = TRUE) Set up Priors So in this example, we will want to set up six prior distributions, because in the above model set-up, the st0 and d have been set to constant as 0. That is, they are deterministic, not stochastic. ggdmc (as well as DMC) has a function to build prior. Unimaginatively, it is called BuildPrior (it is called p.prior.dmc in DMC). p.prior &lt;- BuildPrior( p1 = c(a = 1.5, v = 3, z = .5, sz = .3, sv = 1, t0 = .2), p2 = c(a = 0.5, v = .5, z = .1, sz = .1, sv = .3, t0 =.05), lower = c(0, -5, 0, 0, 0, 0), upper = c(2, 7, 4, 4, 4, 1), dists = rep(\"tnorm\", 6)) A list of options arguments for the BuildPrior function can be found by enter: ?BuildPrior Here is a copy from the R documentation in ggdmc pacakge. p1 simply means the first parameter of a distribution p2 simiarly mean the second parameter of a distribution lower is the lower support (i.e., the lower truncated boundary) upper is the upper support (i.e., the upper truncated boundary) dists is a string vector specifying the name of a distribution. Current version of ggdmc provides four types of prior distributions: tnorm, Normal and truncated normal, where: p1 = mean, p2 = sd. It specifies a normal distribution when bounds are set -Inf and Inf, beta, Beta, where: p1 = shape1 and p2 = shape2 (see ?pbeta in R). Note the uniform distribution is a special case of the beta distribution when p1 and p2 = 1, gamma, Gamma, where p1 = shape and p2 = scale (see ?pgamma in R). Note p2 is scale, not rate, lnorm, Lognormal, where p1 = meanlog and p2 = sdlog (see ?plnorm). In the ggdmc (as well as DMC) operation, the names (i.e., character strings) are important for corret computation. The two options, lower and upper, are to set the distribution support. for tnorm, these define the lower and upper bounds; When the user enters NA , the default behaviour of the function is to set the values as -Inf and Inf. This make a truncated normal distribution becoming a normal distribution (see ?pnorm). for beta, these define the lower and upper bounds (i.e., scaled beta distribution). The default behaviour for entering NA is to filled with the values of 0 and 1. p1 = 1 &amp; p2 = 1 &amp; lower = 0 (default) &amp; upper = 1 (default) creates Uniform(0, 1) p1 = 1 &amp; p2 = 1 &amp; lower = l &amp; upper = u creates Uniform(l, u) for gamma, lower shifts the distribution to exclude small values for lognormal, lower shifts the distribution to exclude small values Example 1: Set up beta (and uniform) prior Currently, the below example is from Heathcote et al’s (2018) DMC tutorial of LNR model. beta.prior &lt;- BuildPrior( dists = c(\"beta\", \"beta\", \"beta\", \"beta\", \"beta\"), p1 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1), p2 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1), lower = c(-4,-4, 0, 0, 0.1), upper = c( 4, 4, 4, 4, 1)) You can plot and print the prior distribution by using the plot and print functions. plot(beta.prior) print(p.prior) ## p1 p2 lower upper log dist untrans ## meanlog.true 1 1 -4 4 1 beta_lu identity ## meanlog.false 1 1 -4 4 1 beta_lu identity ## sdlog.true 1 1 0 4 1 beta_lu identity ## sdlog.false 1 1 0 4 1 beta_lu identity ## t0 1 1 0.1 1 1 beta_lu identity This is how to calculate log-prior likelihoods (i.e., probability densities) for each model parameter and add them all together. dprior(p.vector, p.prior) ## meanlog.true meanlog.false sdlog.true sdlog.false t0 ## -2.0794415 -2.0794415 -1.3862944 -1.3862944 0.1053605 sumlogpriorNV(p.vector, p.prior) ## [1] -6.826111 What to look for when set up prior distributions For setting up prior distributions, key points are to look for first whether prior distributions cover broad range (i.e., relatively uninformative) and second whether their range cover abnormal values. For example, it is not possible to have negative standard devation, so sd_v.true subpanel should not cover negative values."
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-theorem": {
						"id": "bayes-basics-theorem",
						"title": "Bayes' Theorem",
						"category": "",
						"url": " /bayes-basics/theorem/",
						"content": "How do we reach decisions to act on something? One way to answer this question is the action driven by a decision bringing positive feedback. The feedback is often accompanied by monetary or other forms of rewards; it thereby motivates us to make such decisions in the future. In the terminology of psychological experiments, the feedback could conceive as been shown in the form of data, which are collected from human and or animal subjects. In other words, the theories (our prior beliefs) behind every decision that entails an action (prediction) and we then collect data to check how close the prediction fit the data. How? Often the closer our prediction matches the resultant data, the more rewards we might receive. Hence when there are some mismatches between the predictions and the data, we would likely modify our theories beliefs. They then become posterior belief. This intuitive idea of human decision-making is described by the well-known Bayes’ theorem (Bayes, Price, &amp; Canton, 1763): y represents data. For example, a serial of response times in seconds, c(0.533, 0.494, 0.494, …); θ represents a set of parameters. That is, it is a parameter vector; P(θ) represents our prior belief in the form of a probability distribution, which is fully accounted for the parameter vector. This is often dubbed the prior distribution. P(y | θ) represents the mechanism accounting for the data. This is often dubbed (data’s) likelihood function. P(θ | y) represents posterior belief, which similar to the prior belief, is often dubbed the posterior distribution."
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-cddm": {
						"id": "cognitive-model-cddm",
						"title": "Circular Diffusion Decision Model",
						"category": "",
						"url": " /cognitive-model/cddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-ddm": {
						"id": "cognitive-model-ddm",
						"title": "Diffusion Decision Model",
						"category": "",
						"url": " /cognitive-model/ddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-lba": {
						"id": "cognitive-model-lba",
						"title": "Linear Ballistic Accumulation Model",
						"category": "",
						"url": " /cognitive-model/lba/",
						"content": "This lesson demonstrates how to control the “golem” (McElreath, 2016), the canonical linear ballistic accumulation (LBA) model (Brown &amp; Heathcote, 2008). Please refer to the above LBA paper for more details. Here I focus only on how to use this model in the Bayesian MCMC context. The LBA model posits a latent matching (M) factor and a response factor (R) on top of regular experimental factors. For most people who are not familiar with the LBA model, the two factors are unfortunately confusing. Also for the modelling technicalities, the LBA model must fix one of the parameters in the mean_v or sd_v in at least one design cell. This is to serve as scaling purpose, similar to the moment-to-moment variability in the decision diffusion model. For example, the following code fixes sd_v = 1. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\")), constants = c(st0 = 0, sd_v = 1), responses = c(\"r1\", \"r2\"), type = \"norm\") In the above model, I define only one experimental factor, S, for stimulus, which has two levels, s1 and s2. The accuracy, reflected by the M factor, is mapped by s1 = 1 and s2 = 2, meaning that a correct response for s1 (or s2) stimulus is response r1 (or r2) and an error response for s1 (or s2) stimulus is r2 (or r1). Below I use simulate to generate an example data set. p.vector &lt;- c(A = .75, B = 1.25, t0 = .15, mean_v.true = 2.5, mean_v.false = 1.5) dat &lt;- simulate(model, nsim = 30, ps = p.vector) dmi &lt;- BuildDMI(dat, model) ## DMI stands for data model instance. dplyr::tbl_df(dmi) # A tibble: 60 x 3 ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r1 0.608 ## 2 s1 r1 0.972 ## 3 s1 r2 0.817 ## 4 s1 r1 0.718 ## 5 s1 r1 0.618 ## 6 s1 r1 1.17 ## 7 s1 r1 0.730 ## 8 s1 r2 0.727 ## 9 s1 r1 0.711 ## 10 s1 r1 0.688 I use an imaginary experiment with a design of one binary stimulus factor (S), such as left vs. right motion random dots. match.map = list(M = list(left = “LEFT”, right = “RIGHT”)), responses = c(“LEFT”, “RIGHT”), In another tutorial, I will fit the model to an empirical data set (Cox &amp; Criss, 2017) to demonstrate fitting HLBA model. The above match.map code shows the usage of strings, instead of numbers. The “left” and “LFET” could mean the random dots moving left and a left response. From an experimenter’s perspective, this imaginary experiment only has one stimulus (S) factor, which has two levels, random dots moving towards right and moving towards left as defined below. factors = list(S = c(“left”, “right”)), Below is the complete model definition. model &lt;- BuildModel(p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), constants = c(st0 = 0, sd_v.false = 1, mean_v.false = 0), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), responses = c(\"LEFT\", \"RIGHT\"), type = \"norm\") The first option in the BuildModel function, p.map indicates the experimental design. In this example, I assumed the S factor does not affect any LBA latent variables operations. Therefore, I entered p.map as: p.map = list(A = “1”, B = “1”, t0 = “1”, mean_v = “M”, sd_v = “M”, st0 = “1”), The notations of the parameters in the LBA model refer to: A, the variability of the starting point, B, the travelling distance of accumulators, b, (not shown in the p.map) the decision threshold, t0, the non-decision time mean_v, the means of the drift rates, sd_v, the standard deviations of the drift rates, st0, the variability of the non-decision time component. The A = “1”, for instance, indicates that the variability of the starting point is fit by the intercept, 1. The M factor, because it is defined by the LBA model as a latent factor, you still see it in the p.map. It indicates there are two drift rate means in mean_v, one for each accumulator: the accumulator for the correct matched responses and the accumulator for the error mismatched responses. Similarly, this is also applied to the standard deviation of the drift rates, sd_v. The only effect in the model defined in the p.map is that the drift rate for a correct response is larger than that for an error response. This is an assumption based on, in general, psychological literature. This is artificially set at constants = c(st0 = 0, sd_v.false = 1, mean_v.false = 0), which enforces mean_v.false = 0. This is to presume (also frequent observed phenomenon) that manifested accuracy rate should usually be greater than chance (50%). mean_v.false stands for the mean of the drift rate of the error (false) accumulator. Because it is always zero, the correct drift rate, mean_v.true, if drawn from a truncated normal distribution bounded by 0 and Inf, will always be larger than the error drift rate. Demo 1 Fast and error prone performance This demonstration shows how I control the LBA golem to simulate fast and error prone RT distributions. I defined a true parameter vector, defining sd_v.true = (0.66), which is smaller than sd_v.false = 1. This seems often seen in empirical data. pvec1 &lt;- c(A = 1, B = 0, t0 = .2, mean_v.true = 1, sd_v.true = 0.66) dat1 &lt;- simulate(model, ps = pvec1, nsim = 1e4) dmi1 &lt;- BuildDMI(dat1, model) In the following, I used functions in dplyr to print out the mean response times and accuracy for each stimulus types. The results showed: Error and correct responses have similar average RTs. Stimulus type 1 and stimulus type 2 have similar rates of correctness. library(dplyr) ## dplyr library(dplyr) dat1$C &lt;- dat1$S == tolower(dat1$R) d &lt;- dplyr::tbl_df(dat1) ## Print average RTs and accuracy rates for each condition group_by(d, S, C) %&gt;% summarize(m = mean(RT)) ## A tibble: 4 x 3 ## Groups: S [?] ## S C m ## &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 left FALSE 0.624 ## 2 left TRUE 0.645 ## 3 right FALSE 0.639 ## 4 right TRUE 0.634 group_by(d, S, C) %&gt;% summarize(m = length(RT) 1e4) ## A tibble: 4 x 3 ## Groups: S [?] ## S C m ## &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 left FALSE 0.391 ## 2 left TRUE 0.609 ## 3 right FALSE 0.392 ## 4 right TRUE 0.608 ## data.table library(data.table) DT &lt;- data.table(dat1) ## Print average RTs and accuracy for each condition DT[, .(MRT = round(mean(RT), 3)), .(S, C)] ## S C MRT ## 1: left TRUE 0.645 ## 2: left FALSE 0.624 ## 3: right TRUE 0.634 ## 4: right FALSE 0.639 prop &lt;- DT[, .N, .(S, C)] prop[, NN := sum(N), .(S)] prop[, acc := round(N NN, 2)] ## Print accuracy rates for each condition prop ## S C N NN acc ## 1: left TRUE 6092 10000 0.61 ## 2: left FALSE 3908 10000 0.39 ## 3: right TRUE 6079 10000 0.61 ## 4: right FALSE 3921 10000 0.39"
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-lba3": {
						"id": "cognitive-model-lba3",
						"title": "Three-accumulator LBA Model",
						"category": "",
						"url": " /cognitive-model/lba3/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-pddm": {
						"id": "cognitive-model-pddm",
						"title": "PDDM",
						"category": "",
						"url": " /cognitive-model/pddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-plba": {
						"id": "cognitive-model-plba",
						"title": "PLBA Model",
						"category": "",
						"url": " /cognitive-model/plba/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-sdt": {
						"id": "cognitive-model-sdt",
						"title": "Signal Detection Model",
						"category": "",
						"url": " /cognitive-model/sdt/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"fixed-effect-model-many-participants": {
						"id": "fixed-effect-model-many-participants",
						"title": "Multiple Participants",
						"category": "",
						"url": " /fixed-effect-model/many_participants/",
						"content": "In this tutorial, I illustrated fitting multiple participants, assuming the mechanism of data generation is fixed-effect models. That is, each participant is accounted for by independent mechanisms. I also assume the LBA model is the true RT model. I made up a two-factor factorial design. The first two-level factor is the stimulus (S). Suppose the stimuli have two types: one is low quality face photos, so people find it hard to recognize and the other is normal quality face photo. The second factor is the frequency (F), supposing one type is the celebrity photos, so people perhaps see more often, and the other is the photos of randomly selected strangers. In the model set-up, I presume a rate model, which has its drift rates affected by the two factors, S and F. The latent factor, M, is just a LBA way to model independent accumulators. Another factor, R, not explicitly in the factorial design, is an indicator factor, indicating the response type affecting the threshold parameter (i.e., accumulators traveling distance). require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = c(\"S\", \"F\", \"M\"), sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\"), F = c(\"f1\", \"f2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") ## [1] \"A\" \"B.r1\" \"B.r2\" ## [4] \"t0\" \"mean_v.s1.f1.true\" \"mean_v.s2.f1.true\" ## [7] \"mean_v.s1.f2.true\" \"mean_v.s2.f2.true\" \"mean_v.s1.f1.false\" ## [10] \"mean_v.s2.f1.false\" \"mean_v.s1.f2.false\" \"mean_v.s2.f2.false\" ## [13] \"sd_v.true\" npar &lt;- length(GetPNames(model)) To simulate many participants, I set up a population distribution, which is not in line with the assumption of fixed-effects model. That is, this way to generate data is to presume that a random-effects model at work. For the purpose of illustration, I forgo this issue for now. pop.mean &lt;- c(A = .4, B.r1 = .85, B.r2 = .8, t0 = .1, mean_v.s1.f1.true = 2.5, mean_v.s2.f1.true = 3.5, mean_v.s1.f2.true = 4.5, mean_v.s2.f2.true = 5.5, mean_v.s1.f1.false = 1.00, mean_v.s2.f1.false = 1.10, mean_v.s1.f2.false = 1.05, mean_v.s2.f2.false = 1.20, sd_v.true = .25) pop.scale &lt;- c(A = .1, B.r1 = .1, B.r2 = .1, t0 = .05, mean_v.s1.f1.true = .2, mean_v.s2.f1.true = .2, mean_v.s1.f2.true = .2, mean_v.s2.f2.true = .2, mean_v.s1.f1.false = .2, mean_v.s2.f1.false = .2, mean_v.s1.f2.false = .2, mean_v.s2.f2.false = .2, sd_v.true = .1) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 4), rep(NA, 8), 0), upper = c(rep(NA, npar))) We may want to check how the prior distributions look like. ggdmc has a plot function to do just that. Note you need to load ggdmc package (i.e., require(ggdmc)) to make plot function changes its default behaviour. plot(pop.prior) ## Simulate some data dat &lt;- simulate(model, nsim = 30, nsub = 8, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) dplyr::tbl_df(dat) ## # A tibble: 960 x 5 ## s S F R RT ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 s1 f1 r1 0.438 ## 2 1 s1 f1 r1 0.517 ## 3 1 s1 f1 r1 0.407 ## 4 1 s1 f1 r1 0.454 ## 5 1 s1 f1 r1 0.449 ## 6 1 s1 f1 r1 0.463 ## 7 1 s1 f1 r1 0.552 ## 8 1 s1 f1 r1 0.411 ## 9 1 s1 f1 r1 0.387 ## 10 1 s1 f1 r1 0.486 ## # ... with 950 more rows The true averaged parameter vectors, which were randomly chosen based on pop.prior, can be retrieved by looking up the parameters attribute, attached onto the dat object. However, note the real true values are pop.mean and pop.scale, because the data are generated based on random-effects model. require(matrixStats) ps &lt;- attr(dat, \"parameters\") mu &lt;- round(colMeans2(ps), 2) sigma &lt;- round(colSds(ps), 2) truevalues &lt;- rbind(mu, sigma) colnames(truevalues) &lt;- GetPNames(model) ## Set up prior distributions p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*20, lower = c(rep(0, 4), rep(NA, 8), 0), upper = c(rep(NA, npar))) Now we am ready to fit the eight participants. Ideally, this can be done simultaneously, if a eight-core machine is available. Here I used a four-core machine so launched 2 cores only (ncore = 2). ## Sampling ------------- fit0 &lt;- StartNewsamples(dmi, p.prior, ncore = 2) fit &lt;- run(fit0, 5e2, ncore = 2) Use plot to check whether posterior log-likelihood converged. plot(fit) gelman function prints the potential scale reduction factor (psrf). A psrf value less than 1.1 suggests chains are well-mixed. res &lt;- gelman(fit, verbose = TRUE) # Diagnosing theta for many participants separately # 15 20 13 18 6 17 12 19 5 11 8 3 16 14 1 2 9 # 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.02 1.02 # 4 7 10 # 1.02 1.03 1.03 # Mean # [1] 1.01 By setting the option, pll (posterior log-likelihood), to FALSE and the option, den (density plot), to TRUE , we can check the trace plots for each model parameters. Because there are several participants, the size of the figure is considerably large. It may be a better to plot separately in a pdf file and check it later. pdf(\"figs subjects-density.pdf\") lapply(fit, ggdmc:::plot.model, pll = FALSE, den = TRUE ) dev.off() One specific feature in ggdmc is that it uses pMCMC, so occasionally, we want to check a subset of chains. The function will randomly pick three chains to plot plot(sam, subchain = TRUE) You can indicate how many subset of chains to plot, too. plot(sam, subchain = TRUE, nsubchain = 4)) You can also indicate which chains, instead of randomly selecting a subset of chains. plot(sam, subchain = TRUE, nsubchain = 4, chains = c(1:4)) These are a lot of checks! Finally and fortunately, because this is a parameter recovery study, I can look up the true parameters to see if I do estimate them well. summary function will do the trick by entering TRUE for the recovery option and entering the true parameter matrix, which I had stored it to a ps object before, to ps option. est &lt;- summary(sam, recovery = TRUE, ps = ps) # Summary each participant separately # A B.r1 B.r2 t0 mean_v.s1.f1.true mean_v.s2.f1.true mean_v.s1.f2.true # Mean 0.50 1.02 0.96 0.10 3.05 4.34 5.50 # True 0.41 0.82 0.77 0.10 2.49 3.56 4.50 # Diff -0.09 -0.20 -0.19 0.00 -0.56 -0.78 -1.00 # Sd 0.15 0.20 0.17 0.05 0.49 0.44 0.76 # True 0.11 0.10 0.09 0.05 0.21 0.19 0.22 # Diff -0.04 -0.10 -0.08 -0.01 -0.28 -0.25 -0.54 # mean_v.s2.f2.true mean_v.s1.f1.false mean_v.s2.f1.false mean_v.s1.f2.false # Mean 6.69 1.59 1.58 0.70 # True 5.49 1.03 1.15 1.08 # Diff -1.20 -0.56 -0.43 0.38 # Sd 0.82 0.34 1.21 1.58 # True 0.22 0.15 0.23 0.15 # Diff -0.59 -0.19 -0.98 -1.43 # mean_v.s2.f2.false sd_v.true # Mean -0.38 0.30 # True 1.11 0.24 # Diff 1.48 -0.06 # Sd 1.23 0.11 # True 0.16 0.10 # Diff -1.07 -0.01"
					}

					
				
			
		
			
				
					,
					

					"fixed-effect-model-one-participant": {
						"id": "fixed-effect-model-one-participant",
						"title": "One Participant",
						"category": "",
						"url": " /fixed-effect-model/one_participant/",
						"content": "Fixed-effects models refer to a scenario that each participant has independent parameter generating mechanism. This is relative to another scenario that all participants are under one common mechanism of parameter generation. The latter scenario sometimes is dubbed random effects, hierarchical or multi-level models, although each term has slightly different meanings. In this tutorial, I illustrate the method of conducting Bayesian MCMC sampling in the fixed-effects scenario. Given a data set containing (1) response times and (2) response choices, one aim is to estimate the parameters that generate the response latency and choices. The sampling technique based on Bayesian MCMC helps to draw (posterior) samples from the probability distribution generating the data, even we do not know the exact mathematical form of this particular probability distribution. For example, we know the Gaussian (normal distribution) function. If we also know the values of its parameters, mean and standard deviation, we can draw its samples by, for instance, using R’s rnorm function, mu &lt;- 0 sigma &lt;- 1 dat &lt;- rnorm(1e3, mu, sigma) The usual situation is that we would collect data (dat) by inviting participants to visit our lab, having them perform some sort of cognitive tasks and in the meantime recording their RTs and choices. In this more realistic situation, we need to estimate mu and sigma. Of course, this presumes that if we assume that the Gaussian is the model accounting for participants’ particular behaviours when they are doing the cognitive tasks. More often, we would use a RT model, for example diffusion decision model (DDM) (Ratcliff &amp; McKoon, 2008)1. As usual, I firstly set up a model object. The type = “rd”, refers to Ratcliff’s diffusion model. model &lt;- BuildModel( p.map = list(a = \"1\", v = \"1\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), responses = c(\"r1\", \"r2\"), constants = c(st0 = 0, d = 0), type = \"rd\") p.vector &lt;- c(a = 1, v = 1.2, z = .38, sz = .25, sv = .2, t0 = .15) ntrial &lt;- 1e2 dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dmi &lt;- BuildDMI(dat, model) ## A tibble: 200 x 3 ## use dplyr::tbl_df(dat) to print this ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r1 0.249 ## 2 s1 r1 0.246 ## 3 s1 r2 0.262 ## 4 s1 r1 0.519 ## 5 s1 r1 0.205 ## 6 s1 r1 0.177 ## 7 s1 r1 0.174 ## 8 s1 r1 0.378 ## 9 s1 r1 0.197 ## 10 s1 r1 0.224 ## ... with 190 more rows Because the data were simulated from a set of presume true values, p.vector, I can use them later to verify whether the sampling process appropriately estimates the parameters. In Bayesian statistics, we also need prior distributions, so let’s build a set of prior distributions for each DDM parameters. A beta distribution with shape1 = 1 and shape2 = 1, equals to a uniform distribution (beta(1, 1)). This is for the start point, z, its variability sz and t0 parameters. All three are bounded by 0 and 1. Others use truncated normal distributions bounded by lower and upper arguments. plot draws the prior distribution, providing a visual check method. p.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 2), \"beta\", \"beta\", \"tnorm\", \"beta\"), p1 = c(a = 1, v = 0, z = 1, sz = 1, sv = 1, t0 = 1), p2 = c(a = 1, v = 2, z = 1, sz = 1, sv = 1, t0 = 1), lower = c(0, -5, NA, NA, 0, NA), upper = c(5, 5, NA, NA, 5, NA)) plot(p.prior, ps = p.vector) By default StartNewsamples uses p.prior to randomly draw start points and 500 MCMC samples. This step uses a mixture of crossover and migration operators. The run function by default draw 500 MCMC samples, using only crossover operator. gelman function report rhat value of 1.06 in this case. A rhat value less than 1.1 is usually considered an indication of chains converged. fit0 &lt;- StartNewsamples(dmi, p.prior) fit &lt;- run(fit0) rhat &lt;- gelman(fit, verbose = TRUE) ## Diagnosing a single participant, theta. Rhat = 1.06 plot by default draws posterior log-likelihood, with the option, start, to change to a latter start iteration to draw. p0 &lt;- plot(fit0) ## p0 &lt;- plot(fit0, start = 101) p1 &lt;- plot(fit) png(\"pll.png\", 800, 600) gridExtra::grid.arrange(p0, p1, ncol = 1) dev.off() The upper panel showed the chains quickly converged to posterior log-likelihoods near 100th iteration and the right panel confirmed the rhat value (&lt; 1.1). p2 &lt;- plot(fit, pll = FALSE, den= FALSE) p3 &lt;- plot(fit, pll = FALSE, den= TRUE) png(\"den.png\", 800, 600) gridExtra::grid.arrange(p2, p3, ncol = 1) dev.off() In a simulation study, we can check whether the sampling process is OK, using summary est &lt;- summary(fit, recover = TRUE, ps = p.vector, verbose = TRUE) ## a sv sz t0 v z ## True 1.00 0.20 0.25 0.15 1.20 0.38 ## 2.5% Estimate 0.99 0.02 0.01 0.14 1.09 0.32 ## 50% Estimate 1.07 0.41 0.22 0.15 1.45 0.35 ## 97.5% Estimate 1.16 1.18 0.43 0.16 1.81 0.39 ## Median-True 0.07 0.21 -0.03 0.00 0.25 -0.03 Finally, we may want to check whether the model fits the data well. There are many methods to to quantify the goodness of fit. Here, I illustrate two methods. First method is to calculate DIC and BPIC. These information criteria are useful for model selection. (need &gt; ggdmc 2.5.5) DIC(fit) BPIC(fit) Secondly, I simulate post-predictive data, based on the parameter estimates. xlim trims off outlier values in the simulation data. pp &lt;- predict_one(fit, xlim = c(0, 5)) dat$C &lt;- ifelse(dat$S == \"s1\" &amp; dat$R == \"r1\", TRUE, ifelse(dat$S == \"s2\" &amp; dat$R == \"r2\", TRUE, ifelse(dat$S == \"s1\" &amp; dat$R == \"r2\", FALSE, ifelse(dat$S == \"s2\" &amp; dat$R == \"r1\", FALSE, NA)))) pp$C &lt;- ifelse(pp$S == \"s1\" &amp; pp$R == \"r1\", TRUE, ifelse(pp$S == \"s2\" &amp; pp$R == \"r2\", TRUE, ifelse(pp$S == \"s1\" &amp; pp$R == \"r2\", FALSE, ifelse(pp$S == \"s2\" &amp; pp$R == \"r1\", FALSE, NA)))) dat$reps &lt;- NA dat$type &lt;- \"Data\" pp$reps &lt;- factor(pp$reps) pp$type &lt;- \"Simulation\" DT &lt;- rbind(dat, pp) p1 &lt;- ggplot(DT, aes(RT, color = reps, size = type)) + geom_freqpoly(binwidth = .05) + scale_size_manual(values = c(1, .3)) + scale_color_grey(na.value = \"black\") + theme(legend.position = \"none\") + facet_grid(S ~ C) d &lt;- data.table::data.table(dat) d[, .N, .(S, R)] ## S R N ## 1: s1 r1 87 ## 2: s1 r2 13 ## 3: s2 r1 34 ## 4: s2 r2 66 The grey lines are model predictions. By default, predict_one randomly draws 100 parameter estimates and simulate data based on them. Therefore, there are 100 lines, showing the prediction variability. The solid dark line is the data, in the case, appropriately fall within the range covering by the grey lines. Note that the error responses (FALSE) are not predicted as well as the correct responses. This is fairly common, when the number of trial is minimal. In this case, it has only 13 trials. This is often dubbed, drift-diffusion model, but in Ratcliff and McKoon’s work, they called it diffusion decision model. &#8617;"
					}

					
				
			
		
			
				
					,
					

					"mcmc-hastings": {
						"id": "mcmc-hastings",
						"title": "Metropolis-Hastings",
						"category": "",
						"url": " /mcmc/hastings/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"mcmc-mcmc": {
						"id": "mcmc-mcmc",
						"title": "Markov Chain Monte Carlo",
						"category": "",
						"url": " /mcmc/mcmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"mcmc-rwm": {
						"id": "mcmc-rwm",
						"title": "Random Walk Metropolis",
						"category": "",
						"url": " /mcmc/rwm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hcddm": {
						"id": "random-effect-model-hcddm",
						"title": "Hierarchical Circular DDM",
						"category": "",
						"url": " /random-effect-model/hcddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hddm": {
						"id": "random-effect-model-hddm",
						"title": "Hierarchical DDM",
						"category": "",
						"url": " /random-effect-model/hddm/",
						"content": "In this tutorial, I conducted a parameter recovery study, demonstrating the pMCMC method to fit a hierarchical DDM for a relatively simple factorial design. Set-up a model object This particular design is drawn from Heathcote et al’s (2018) DMC tutorial, which assumes that a word frequency (my interpretation) factor affecting the mean drift rate (v). Note it is not a good practice to use “F” notation in R, because it is also used as a shorthand for the reserved word, meaning FALSE. However, one of the R strengths is it permits idiosyncratic programming habits, even bad ones. library(ggdmc) model &lt;- BuildModel( p.map = list(a = \"1\", v = \"F\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\"), F = c(\"f1\", \"f2\")), constants = c(st0 = 0, d = 0), responses = c(\"r1\", \"r2\"), type = \"rd\") npar &lt;- length(GetPNames(model)) To conduct a parameter recovery study, I firstly assumed a hidden multi-level mechanism generating the data. That is, I presumed there is a distribution at the population participants level. This distribution is a 7 dimension distribution, which has 7 marginal distributions. Each of them is in control of one DDM parameter. pop.mean &lt;- c(a = 2, v.f1 = 4, v.f2 = 3, z = .5, sz = .3, sv = 1, t0 = .3) pop.scale &lt;- c(a = .5, v.f1 =.5, v.f2 = .5, z = .1, sz = .1, sv = .3, t0 = .05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(0, rep(-5, 2), rep(0, 4)), upper = c(5, rep( 7, 2), 1, 2, 1, 1)) As usual, I want to visually check if the assumed mechanism is reasonable. plot(pop.prior) After making sure that the data generating mechanism is proper, I then simulated a data set with 40 participants and 250 trials for each condition. dat &lt;- simulate(model, prior = pop.prior, nsim = 250, nsub = 40) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") In the hierarchical case of the parameter recovery study, my aim is to recover not only ps matrix, but also the data generating mechanism. That is, I wish to be able to known pop.mean, pop.scale and their marginal distribution, as well as the ps. A reminder ps is a matrix, storing the true values for each DDM parameter. Each row of the matrix represents the parameter vector for a participant. dplyr::tbl_df(ps) ## A tibble: 40 x 7 ## a v.f1 v.f2 z sz sv t0 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.93 4.30 2.95 0.486 0.151 0.775 0.304 ## 2 1.52 3.74 2.40 0.589 0.258 0.809 0.340 ## 3 1.85 4.31 2.62 0.636 0.318 0.903 0.349 ## 4 2.34 3.94 2.53 0.578 0.127 0.993 0.283 ## 5 2.44 4.50 2.68 0.566 0.246 0.589 0.261 ## 6 2.73 4.08 3.68 0.465 0.182 0.973 0.290 ## 7 2.34 2.89 3.69 0.742 0.307 0.592 0.325 ## 8 1.90 4.35 3.56 0.512 0.186 0.513 0.309 ## 9 2.60 4.18 2.81 0.541 0.367 0.758 0.270 ## 10 1.64 3.77 2.37 0.596 0.286 0.995 0.375 ## with 30 more rows OK. The above is only preparation work for a parameter recovery study. In the following, I will start to conduct Bayesian sampling to draw samples from the posterior distribution, hoping that I can recover pop.mean, pop.scale, the target distribution, and the ps matrix. I already have the likelihood, which is the DDM equation (Ratcliff &amp; Tuerlinckx, 2002). I will need to set up my prior belief, namely prior distributions for the seven DDM parameters. In the case of hierarchical model, there are two sets of prior distributions: one is usually called hyper-prior distributions and the other simply prior distributions. The former is my prior belief about the distributions (pp.prior below) accounted by pop.mean and pop.scale. The latter is my prior belief about separate DDM mechanisms for each individual participant (p.prior below). p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,-5, -5, rep(0, 4)), upper = c(5, 7, 7, 1, 2, 1, 1)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,-5, -5, rep(0, 4)), upper = c(5, 7, 7, 1, 2, 1, 1)) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar), upper = rep(2, npar)) names(sigma.prior) &lt;- GetPNames(model) A convention in ggdmc is to bind location and scale prior distributions as one list object. This is just for the convenience of data handling in R, which is not so convenient in C++. priors &lt;- list(pprior = p.prior, locaton = mu.prior, scale = sigma.prior) Then, the following run sampling. ## run the \"?\" to see the details of function options ?StartNewHypersamples ?run hsam0 &lt;- run(StartNewHypersamples(5e2, dmi, p.prior, pp.prior, 2), pm = .05, hpm = .05) ## 35 mins hsam &lt;- run(RestartHypersamples(5e2, hsam0, thin = 8), pm = 0, hpm = 0) ## 150 mins hsam &lt;- run(RestartHypersamples(5e2, hsam0, thin = 16), pm = 0, hpm = 0) ## 5 hrs hsam &lt;- run(RestartHypersamples(5e2, hsam0, thin = 32), pm = 0, hpm = 0) ## 10 hrs hsam &lt;- run(RestartHypersamples(5e2, hsam0, thin = 64), pm = 0, hpm = 0) ## 20.6 hrs save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, hsam0, hsam, file = \"data hierarchical ggdmc_4_7_DDM.rda\") ## 4 rounds thin &lt;- 8 repeat { hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = thin), pm = 0, hpm = 0) save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, hsam0, hsam, file = \"data hierarchical ggdmc_4_7_DDM.rda\") rhats &lt;- hgelman(hsam) thin &lt;- thin * 2 if (all(rhats &lt; 1.1)) break } save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, hsam0, hsam, file = \"data hierarchical ggdmc_4_7_DDM.rda\") Similar to many standard modeling works, I must diagnose the models so as to make sure I drew a reliable posterior distribution, reflecting the target distribution. I can check visually as well as calculating some statistics. First, I conducted visually checks for the trace plots and posterior distributions. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Trace plots of posterior log-likelihood at the data level Trace plots of each DDM parameters for each participants Posterior density plots (i.e., marginal posterior distributions) for the hyper parameters Posterior density plots the DDM parameters for each parameters plot(hsam, hyper = TRUE) ## 1. plot(hsam, hyper = TRUE, pll = FALSE) ## 2. plot(hsam) ## 3. plot(hsam, pll = FALSE) ## 4. plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) ## 5. plot(hsam, pll = FALSE, den = TRUE) ## 6. These are a lot of figures to check. I have not presented the figures of posterior probability density for every participant (i.e., figure 6), because there are too many. You can print them in a pdf file to check. Then, I calculated the potential scale reduction factor, for both the hyper parameters and each participant. rhat &lt;- hgelman(hsam) ## Diagnosing theta for many participants separately ## Diagnosing the hyper parameters, phi ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 ## 1.02 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 13 14 15 16 17 18 19 20 21 22 23 24 25 ## 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 26 27 28 29 30 31 32 33 34 35 36 37 38 ## 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 39 40 ## 1.01 1.01 Finally, I want to know if I do recover the mechanism of data generation and true parameter values for every participant (i.e., ps). This can be achieved by the summary function. hest1 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.mean, type = 1) hest2 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.scale, type = 2) round(hest1, 2) round(hest2, 2) ests &lt;- summary(hsam, recovery = TRUE, ps = ps) ## a sv sz t0 v.f1 v.f2 z ## True 2.00 1.00 0.30 0.30 4.00 3.00 0.50 ## 2.5% Estimate 1.83 0.69 0.22 0.28 3.69 2.76 0.49 ## 50% Estimate 2.00 0.88 0.29 0.29 3.87 2.93 0.52 ## 97.5% Estimate 2.17 0.99 0.34 0.31 4.07 3.12 0.55 ## Median-True 0.00 -0.12 -0.01 -0.01 -0.13 -0.07 0.02 ## a sv sz t0 v.f1 v.f2 z ## True 0.50 0.30 0.10 0.05 0.50 0.50 0.10 ## 2.5% Estimate 0.43 0.15 0.03 0.03 0.38 0.41 0.08 ## 50% Estimate 0.53 0.30 0.07 0.04 0.49 0.53 0.10 ## 97.5% Estimate 0.71 0.58 0.14 0.05 0.65 0.68 0.13 ## Median-True 0.03 0.00 -0.03 -0.01 -0.01 0.03 0.00 ## Summary each participant separately ## a v.f1 v.f2 z sz sv t0 ## Mean 2.00 3.87 2.94 0.52 0.29 0.72 0.29 ## True 1.98 3.86 2.94 0.52 0.28 0.78 0.29 ## Diff -0.02 -0.01 0.00 0.00 -0.01 0.06 0.00 ## Sd 0.51 0.44 0.49 0.10 0.04 0.15 0.04 ## True 0.52 0.47 0.51 0.10 0.09 0.19 0.04 ## Diff 0.00 0.02 0.02 0.00 0.05 0.04 0.00"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hlba": {
						"id": "random-effect-model-hlba",
						"title": "Hierarchical LBA Model",
						"category": "",
						"url": " /random-effect-model/hlba/",
						"content": "In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit a hierarchical LBA model. This particular design has a stimulus (S) and a frequency (F) factor. Set-up Model I used the usual DMC-style syntax to set up the model. I hope, at this point, it might be clear for the reader who has read through the Model Array, that the meaning of syntax in p.map. p.map = list(A = “1”, B = “R”, t0 = “1”, mean_v = c(“F”, “M”), sd_v = “M”, st0 = “1”), library(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = c(\"F\", \"M\"), sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1=1, s2=2)), factors = list(S = c(\"s1\", \"s2\"),F = c(\"f1\", \"f2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") ## Parameter vector names (unordered) are: ( see attr(,\"p.vector\") ) ## [1] \"A\" \"B.r1\" \"B.r2\" \"t0\" ## [5] \"mean_v.f1.true\" \"mean_v.f2.true\" \"mean_v.f1.false\" \"mean_v.f2.false\" ## [9] \"sd_v.true\" ## ## Constants are (see attr(,\"constants\") ): ## sd_v.false st0 ## 1 0 ## ## Model type = norm (posdrift = TRUE ) npar &lt;- length(GetPNames(model)) mean_v = c(“F”, “M”), refers to that the mean of the drift rate is affected by the F and M factors. The former is an experimental factor and the latter is a latent LBA specific factor. B = “R” refers to that the travel distance parameter is affected by the response factor, which in a binary decision task, is two levels. Here it is either “r1” or “r2”, defined in the response option argument. responses = c(“r1”, “r2”), Therefore, the data.frame (an R way to store real or simulated data ) should have an R column, similar to the below: &gt; dplyr::tbl_df(dat) # A tibble: 40,000 x 5 s S F R RT &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 1 s1 f1 r1 0.745 2 1 s1 f1 r1 0.883 3 1 s1 f1 r1 0.884 4 1 s1 f1 r1 0.678 5 1 s1 f1 r1 0.729 6 1 s1 f1 r1 0.803 7 1 s1 f1 r1 0.735 8 1 s1 f1 r1 0.756 9 1 s1 f1 r1 0.847 10 1 s1 f1 r1 0.855 # ... with 39,990 more rows s refers to subject label, S is stimulus factor, F is frequency factor, R is response factor, fct refers to factor, namely a categorical variable, dbl refers to double, namely a continuous, numerical variable. In this design, the S factor does not affect any model parameters, and the F factor affects the mean of the drift rate, mean_v = c(“F”, “M”). Because this is a parameter recovery study, I simulated a data set based on the above model. Similarly, I used a random-effects model to generate the data set, so I defined a set of population distribution for each LBA parameters. The names of the parameter were reported by the BuildModel function. Simulate Data pop.mean &lt;- c(A=.4, B.r1=.6, B.r2=.8, t0=.3, mean_v.f1.true=1.5, mean_v.f2.true=1, mean_v.f1.false=0, mean_v.f2.false=0, sd_v.true = .25) pop.scale &lt;-c( rep(.1, 3), .05, rep(.2, 4), .1) names(pop.scale) &lt;- names(pop.mean) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 3), .1, rep(NA, 4), 0), upper = c(rep(NA, 3), 1, rep(NA, 5))) dat &lt;- simulate(model, nsim = 30, nsub = 40, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") Then I set up prior distributions and hyper-prior distributions. ### FIT RANDOM EFFECTS p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,0,0,.1,NA,NA,NA,NA,0), upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = c(1,1,1,1,2,2,2,2,1), lower = c(0,0,0,.1,NA,NA,NA,NA,0), upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA)) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = c(A=1, B.r1=1, B.r2=1, t0=1, mean_v.f1.true=1, mean_v.f2.true=1, mean_v.f1.false=1, mean_v.f2.false=1, sd_v.true = 1), p2 = rep(1, npar)) ###### Must names \"pprior\", \"location\", and \"scale\" to the prirors list ####### priors &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) ###### Must names \"pprior\", \"location\", and \"scale\" to the prirors list ####### Sampling Next, I started the sampling. When the debug argument is set TRUE, the run function uses the conventional DE-MCMC sampler, with the its original migration operator. fit0 &lt;- StartNewsamples(dmi, priors) fit &lt;- run(fit0, thin = 8) Model Diagnosis Next, I checked if the model has converged and analyze the model estimation. In this tutorial, I did the numerical checks firstly by calculating the potential scale reduction factors (Brook &amp; Gelman,1998). All are less than 1.05, suggesting all chains are well mixed. rhat &lt;- hgelman(fit, verbose = TRUE) ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 1.02 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 28 29 30 31 32 33 34 35 36 37 38 39 40 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 Then, I calculated effective samples at the hyper parameters, for one participant at the parameter of the data level, and similarly for all participants. This is to check if enough posterior samples are drawn. hes &lt;- effectiveSize(hsam, hyper = TRUE) ## ## A.h1 B.r1.h1 B.r2.h1 t0.h1 ## 4655 1416 1152 2061 ## mean_v.f1.true.h1 mean_v.f2.true.h1 mean_v.f1.false.h1 mean_v.f2.false.h1 ## 1748 2828 1504 1505 ## sd_v.true.h1 A.h2 B.r1.h2 B.r2.h2 ## 3997 3406 3611 2630 ## t0.h2 mean_v.f1.true.h2 mean_v.f2.true.h2 mean_v.f1.false.h2 ## 4906 5329 4615 4342 ## mean_v.f2.false.h2 sd_v.true.h2 ## 4481 3887 es1 &lt;- effectiveSize(hsam[[1]]) ## A B.r1 B.r2 t0 mean_v.f1.true ## 13531 2819 2629 6100 3755 ## mean_v.f2.true mean_v.f1.false mean_v.f2.false sd_v.true ## 3793 4983 4239 6572 es &lt;- effectiveSize(hsam) round(apply(data.frame(es), 1, mean)) round(apply(data.frame(es), 1, sd)) round(apply(data.frame(es), 1, max)) round(apply(data.frame(es), 1, min)) ## A B.r1 B.r2 t0 mean_v.f1.true ## Mean 11159 2972 2726 6577 4055 ## SD 2728 295 172 903 562 ## MAX 13967 3778 3196 8183 5273 ## MIN 4525 2382 2331 4581 3092 ## mean_v.f2.true mean_v.f1.false mean_v.f2.false sd_v.true ## 3654 4793 4683 8110 ## 598 435 455 1748 ## 5056 5582 5699 13163 ## 2647 3960 3865 5830 Then I did the visual check by plotting the six types of trace and density plots. Firstly, I plot the hyper parameters. By entering TRUE to the option, save, you save the data in a data.table format1, for further processing. For instance, you can change the figure to fit the publication requirement. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Posterior density plots the hyper parameters plot(hsam, hyper = TRUE) DT1 &lt;- plot(hsam, hyper = TRUE, pll = FALSE, save = TRUE) plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) Next I checked the trace plots of the posterior log-likelihood and each of the LBA parameters at the data level. Trace plots of posterior log-likelihood at the data level Trace plots of the LBA parameters for each participants plot(hsam) plot(hsam, pll = FALSE) Last, I checked the (posterior) density plots at the data level. There are too many density plots for each participants (nsubject x nparameter = 360), so I did not present them here. Posterior density plots the LBA parameters for each parameters Because this is a parameter recovery study, in the following, I used summary function to check whether Bayesian estimates do recover the true parameters of all participant as well as the mechanism of data generation, namely, pop.mean and pop.scale. There are five arguments in the summary function you need to know to trigger the smart parameter recovery computation. First is hyper = TRUE, which calculate the phi array matrix, which stores hyper parameters. Second is the recovery = TRUE, which informs the function to look for a true parameter vector, which you should enter it at ps argument (ps = pop.mean). Otherwise, the function will throw an error message, complaining that it cannot find the true parameter vector. est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1) ## Error in summary_recoverone(samples, start, end, ps, digits, verbose) : ## Names of p.vector do not match parameter names in samples The fourth argument is type = 1. This is a hyper parameter recovery specific. This is to recover the location (mostly mean) parameters. When type = 2, the function will attempt to recover the scale parameters, which mostly refer to standard deviations. The last useful argument is verbose = TRUE, printing message. est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1) ## No print, the estimates are stored in est1. est2 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.scale, type = 2, verbose = TRUE) ## Storing estmates in est2 and print results rounding to the second digits ## A B.r1 B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false ## True 0.10 0.10 0.10 0.20 0.20 0.20 ## 2.5% Estimate 0.10 0.09 0.02 0.15 0.17 0.19 ## 50% Estimate 0.14 0.12 0.06 0.22 0.21 0.26 ## 97.5% Estimate 0.19 0.16 0.10 0.30 0.27 0.34 ## Median-True 0.04 0.02 -0.04 0.02 0.01 0.06 ## mean_v.f2.true sd_v.true t0 ## True 0.20 0.10 0.05 ## 2.5% Estimate 0.21 0.11 0.03 ## 50% Estimate 0.26 0.14 0.04 ## 97.5% Estimate 0.34 0.21 0.06 ## Median-True 0.06 0.04 -0.01 By checking the Median-True, namely 50% quantile minus true values, I can confirm that I did recover the true hyper scale parameters. The lines, 2.5% Estimate and 97.5% Estimate inform that the 95% credible intervals, cover the true hyper parameters well. As expected the A and B parameters are sometimes at the boundaries of the 95% intervals, as they are linearly correlated. More often, researchers concern the location estimates, which I printed out the results stored in est1. A B.r1 B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false True 0.40 0.60 0.80 0.00 1.50 0.00 2.5% Estimate 0.35 0.61 0.83 0.00 1.47 0.00 50% Estimate 0.40 0.68 0.91 0.16 1.58 0.17 97.5% Estimate 0.45 0.76 0.99 0.31 1.68 0.32 Median-True 0.00 0.08 0.11 0.16 0.08 0.17 mean_v.f2.true sd_v.true t0 True 1.00 0.25 0.30 2.5% Estimate 1.04 0.17 0.26 50% Estimate 1.14 0.25 0.28 97.5% Estimate 1.25 0.30 0.30 Median-True 0.14 0.00 -0.02 The estimates look pretty good. My estimation correctly reflects the difference of the two threshold-related parameters. The true values are B.r1 = .6 and _B.r2 = .8. The estimates are B.r1 = .68 and _B.r2 = .91 (B.r2 &gt; B.r1). Even the difference is very close (0.20 vs. 0.23). As expected, the mean drift rates for the error accumulators (e.g., mean_v.f1.false) are very difficult to estimate, because I set my prior distributions belief truncated at the 0 boundary, reflect how prior belief may affect the estimate. More important is the estimates of the mean drift rate for the correct accumulators (e.g., mean_v.f1.true). Again the condition f1 (e.g., high word frequency) has faster drift than the condition f2 (e.g., low word frequency). The estimates, f1 = 1.58 &gt; f2 = 1.14, closely match the true values f1 = 1.50 &gt; f2 = 1.00. There are more useful options in the summary function, which I will return to in a later tutorial. hest &lt;- summary(hsam, hyper = TRUE, hmeans = TRUE) hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c(\"25%\", \"75%\")) hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c(\"25%\", \"75%\"), digits = 3) Posterior predictive check You can also pipe the result to DMC to use its h.post.predict.dmc to conduct posterior predictive check at the hyper parameter. Note this will take a while, because piping back to DMC means using R language to process large Bayesian MCMC data. Further tutorials for DMC, please refer to Heathcote and colleagues (2018). setwd(\" media yslin MERLIN Documents DMCpaper \") source (\"dmc dmc.R\") load_model (\"LBA\",\"lba_B.R\") setwd(\" media yslin MERLIN Documents ggdmc_paper \") hpp &lt;- h.post.predict.dmc(hsam) plot.pp.dmc(hpp) tmp &lt;- lapply(hpp, function(x){plot.pp.dmc(x, style = \"cdf\") }) Reference Heathcote, A., Lin, Y.-S., Reynolds, A., Strickland, L., Gretton, M. &amp; Matzke, D., (2018). Dynamic model of choice. Behavior Research Methods. https: doi.org 10.3758 s13428-018-1067-y. a different way to store and manipulation data in R. &#8617;"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-shooting-decision1": {
						"id": "random-effect-model-shooting-decision1",
						"title": "Shooting Decision Model - Recovery Study",
						"category": "",
						"url": " /random-effect-model/shooting-decision1/",
						"content": "Pleskac, Cesario and Johnson (2017) examined a thorny issue in U.S.A (Wong, 2016; Chinese Community Reels After Brooklyn NYPD Shooting, 2014; American’s police on trial, 2014), whether a police officer’s decision, to shoot or not to shoot, is affected by the race of a shooting target and many other related factors. This is an important question that cognitive experiments might be able to provide some insights. They analyzed four data sets with the hierarchical Wiener diffusion model. They kindly provide their data and JAGS codes at their project OSF. You may also want to read their article, which describes the findings. A previous study, fitting data from also a first-person-shooter task with fixed-effect DDM, is reported in Correll, Wittenbrink, Crawford and Sadler (2015). In this tutorial, I used the data in Pleskac, Cesario and Johnson (2017) to demonstrate how to use pMCMC to fit hierarchical Wiener diffusion to empirical data, although one might account for the decision scenario more relatistically by using the urgency-gating model (Cisek, Puskas, &amp; El-Murr, 2009). That says, I did not argue the urgency- gating model is better than the HDDM, as the story about the LCA model suggested (Miletic, Turner, Forstmann, &amp; van Maanen, 2017). Only when we put them into tests can we know better whether the urgency-gating model is better than the HDDM to account for the shooting decisions. In the case of fitting empirical data I need to rely on other techniques, for example posterior predective check (Gelman, Carlin, Stern, Dunson, Vehtari, &amp; Rubin, 2014), to check whether posterior distributions appropriately reflect target distributions, because I do not know the true data-generation mechanism. Set-up a model object First, I defined a race-threshold model,based on the stereotype findings in classic social psychology literature (Duncan, 1976; Sagar &amp; Schofield, 1990). One question related to this classic observation is that whether this stereotype affects people’s decision threshold, their decision rate, or both. In the first model,I set up a race-threshold model as model1. Below is a list of the abbreviations for each experimental factor. RACE: the stimulus shows a African American (A) vs. a European American (E) S: the stimulus shows one holding a gun (G) or other object (N, not a gun) R: response to “shoot” or “not” to shoot Below is a list of the abbrevations of the DDM parameters. a: the boundary separation v: the mean of the drift rate z: the mean of the starting point of the diffusion relative to threshold separation d: differences in the non-decisional component between upper and lower threshold sz: the width of the support of the distribution of zr sv: the standard deviation of the drift rate t0: the mean of the non-decisional component of the response time st0: the width of the support of the distribution of t0 a = RACE refers to the model assumes that the race factor affects the a parameter. In R style formula, this may look like, a ~ RACE. match.map = list(M = list(G = “shoot”, N = “not”)), stands for that when a trial records a stimuls type as “G”, and a response of “shoot”, it will be coded (by BuildModel) as correct response (TRUE), otherwise error response (FALSE). factors = list(S = c(“G”, “N”), RACE = c(“E”, “A”)), stands for that experiment has two factors, S and RACE, each of them has two levels. The former has a G level for holding a gun object, and a N level for holding a nongun object and the latter has a E level for European American and and A level for African American, levels. library(ggdmc) model &lt;- BuildModel( p.map = list(a = \"RACE\", v = \"S\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(G = \"shoot\", N = \"not\")), factors = list(S = c(\"G\", \"N\"), RACE = c(\"E\", \"A\")), constants = c(st0 = 0, d = 0), responses = c(\"shoot\", \"not\"), type = \"rd\") npar &lt;- length(GetPNames(model)) I conducted a parameter recovery study to certain that the model can fit the data properly beforce I fit to the real data. ## Population distribution pop.mean &lt;- c(a.E = 1.5, a.A = 2.5, v.G = 3, v.N = 2, z = .5, sz = .3, sv = 1, t0 = .2) pop.scale &lt;- c(a.E =.5, a.A = .8, v.G = .5, v.N = .5, z = .1, sz = .1, sv = .3, t0=.05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(5, 2), rep(7, 2), rep(2, 4))) As usual, I want to visually check if the assumed mechanism is reasonable. plot(pop.prior) I loaded the empirical data to see the trial number and participant number in the empirical data. load(\"data race study3.rda\") dplyr::tbl_df(study3) ## A tibble: 12,033 x 7 ## RT S B CT RACE R s ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.753 gun blur safe black not 11 ## 2 0.851 non blur safe white shoot 11 ## 3 0.742 gun clear safe black not 11 ## 4 0.636 non clear safe white shoot 11 ## 5 0.644 gun blur safe black shoot 11 ## 6 0.625 non clear safe black not 11 ## 7 0.889 non clear safe white shoot 11 ## 8 0.597 gun blur safe black not 11 ## 9 0.724 gun clear safe white not 11 ## 10 0.656 non blur safe white shoot 11 ## ... with 12,023 more rows By using the internal function .N in data.table, I knew the actual trial number in a design cell is very small (6 to 33 trials) in study 3. One more reason that I tested only RACE and S factor, which give trial number between 50 to 93. Later, I will tested another model, which also has B and CT factors, to see whether the HDDM still returns good estimates when the trial numbers are very small. s: subject id S: stimulus factor B: object factor: blurrd or clear view CT: context factor: safe or dangerous neighbor study3[, .N, .(s, S, B, CT, RACE)] ## ## s S B CT RACE N ## 1: 11 gun blur safe black 22 ## 2: 11 non blur safe white 23 ## 3: 11 gun clear safe black 19 ## 4: 11 non clear safe white 18 ## 5: 11 non clear safe black 21 ## --- ## 604: 348 gun clear danger black 17 ## 605: 348 non clear danger white 28 ## 606: 348 gun blur danger white 20 ## 607: 348 non clear danger black 16 ## 608: 348 gun blur danger black 23 range(study3[, .N, .(s, S, B, CT, RACE)]$N) ## [1] 6 33 study3[, .N, .(s, S, RACE)] # s S RACE N # 1: 11 gun black 82 # 2: 11 non white 82 # 3: 11 non black 78 # 4: 11 gun white 78 # 5: 19 gun white 83 # --- # 148: 344 non black 84 # 149: 348 non white 93 # 150: 348 gun black 81 # 151: 348 gun white 79 # 152: 348 non black 67 range(study3[, .N, .(s, S, RACE)]$N) ## [1] 50 93 nrow(study3[, .N, .(s)]) ## 38 subjects Then I set up the same number of subjects participants and an optimistic trial numbers. nsubject &lt;- 38 ntrial &lt;- 100 dat &lt;- simulate(model, nsim = ntrial, nsub = nsubject, p.prior = pop.prior) dmi &lt;- BindDataModel(dat, model) ps &lt;- attr(dat, \"parameters\") Next I started the sampling. This will take a few hours. ## Set-up Priors -------- p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*10, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(10, 2), rep(7, 2), rep(5, 4))) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*10, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(10, 2), rep(7, 2), rep(5, 4))) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar), upper = rep(2, npar)) names(sigma.prior) &lt;- GetPNames(model) pp.prior &lt;- list(mu.prior, sigma.prior) ## Sampling ---------- hsam &lt;- run(StartNewHypersamples(5e2, dmi, p.prior, pp.prior, 16), pm = .3, hpm = .3) hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) ## 3 hrs hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 16), pm = .3, hpm = .3) ## 90 mins save(model, pop.prior, nsubject, ntrial, dat, dmi, hsam, file = \"data hierarchical shoot-decision-recovery.rda\") As usual, I checked the model so as to know it is reliable. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Trace plots of posterior log-likelihood at the data level Trace plots of each DDM parameters for each participants Posterior density plots (i.e., marginal posterior distributions) for the hyper parameters Posterior density plots the DDM parameters for each parameters Brook and Gelman potential scale reduction factors plot(hsam, hyper = TRUE) ## 1. plot(hsam, hyper = TRUE, pll = FALSE) ## 2. plot(hsam) ## 3. plot(hsam, pll = FALSE) ## 4. plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) ## 5. plot(hsam, pll = FALSE, den = TRUE) ## 6. rhat &lt;- hgelman(hsam) ## 7. ## Diagnosing theta for many participants separately ## Diagnosing the hyper parameters, phi ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 1.06 1.00 1.00 1.00 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 30 31 32 33 34 35 36 37 38 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.03 Then, I checked if my race-threshold model can recover the parameters. hest1 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.mean, type = 1) hest2 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.scale, type = 2) ests &lt;- summary(hsam, recovery = TRUE, ps = ps) ## Summary each participant separately ## a.E a.A v.G v.N z sz sv t0 ## Mean 1.58 2.61 4.12 3.00 0.48 0.32 1.02 0.22 ## True 1.69 2.50 2.94 2.08 0.51 0.33 1.02 0.19 ## Diff 0.11 -0.11 -1.18 -0.92 0.03 0.00 0.00 -0.02 ## Sd 0.49 0.80 0.39 0.59 0.10 0.12 0.39 0.05 ## True 0.46 0.82 0.44 0.51 0.10 0.09 0.35 0.05 ## Diff -0.04 0.02 0.05 -0.08 -0.01 -0.03 -0.04 0.00 lapply(list(hest1, hest2), round, 2) ## Mean ## a.A a.E sv sz t0 v.G v.N z ## True 2.50 1.50 1.00 0.30 0.20 3.00 2.00 0.50 ## 2.5% Estimate 2.32 1.39 0.58 0.09 0.20 3.88 2.75 0.44 ## 50% Estimate 2.60 1.57 0.98 0.31 0.22 4.11 3.00 0.48 ## 97.5% Estimate 2.89 1.75 1.18 0.39 0.23 4.36 3.25 0.51 ## Median-True 0.10 0.07 -0.02 0.01 0.02 1.11 1.00 -0.02 The recovery study supports the hypothesis that the race-threshold model can recover the parameters reliably. Therefore, if this model is a true model, we can use it to confirm or reject the hypothesis that a police office has a higher shoot threshold towards a black than a white target (i.e., a.A &gt; a.E). Both at the level of individual participants and at the level of hyper parameters. Of course, this presumes that the hierarchical race-threshold model is an appropriate model to better reflect the true phenomenon. That is, if the data do reflect this hypothesis, my hierarchical DDM is able to reveal it. I will return to the technique of model selection in a later tutorial. Note without highly efficient software, like ggdmc, the model selection work is very difficult to conduct. Another strength in ggdmc (as well as DMC), relative to the Python-HDDM (Wiecki, Sofer &amp; Frank, 2013) is that ggdmc estimates the DDM variabilities at the hyper level 1. I will return this particular strength of ggdmc in another tutorial. ## SD ## a.A a.E sv sz t0 v.G v.N z ## True 0.80 0.50 0.30 0.10 0.05 0.50 0.50 0.10 ## 2.5% Estimate 0.67 0.41 0.33 0.10 0.04 0.34 0.50 0.09 ## 50% Estimate 0.85 0.52 0.50 0.17 0.05 0.52 0.67 0.11 ## 97.5% Estimate 1.14 0.71 0.85 0.32 0.07 0.74 0.90 0.14 ## Median-True 0.05 0.02 0.20 0.07 0.00 0.02 0.17 0.01 Now I am ready to fit the empirical data with the race-threshold model. Reference Pleskac, T.J., Cesario, J. &amp; Johnson, D.J. (2017). How race affects evidence accumulation during the decision to shoot. Psychonomic Bulletin &amp; Review, 1-30. https: doi.org 10.3758 s13423-017-1369-6 Wong, J. C. (2016, Aprial, 18). ‘Scapegoated?’ The police killing that left Asian Americans angry – and divided. The Guardian. Chinese Community Reels After Brooklyn NYPD Shooting. (2014, December 24). NBC News. American’s police on trial. (2014, December, 11). The Economist. If one wishes to estimate the DDM variabilities at the hyper level in Python-HDDM, s he would need to modify the Python-HDDM source codes, which is possible but less convenient. &#8617;"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-shooting-decision2": {
						"id": "random-effect-model-shooting-decision2",
						"title": "Shooting Decision Model - Empirical Data",
						"category": "",
						"url": " /random-effect-model/shooting-decision2/",
						"content": "I continued the shooting decision model by fitting the empirical data in study 1 in Pleskac, Cesario and Johnson (2017). First I started from the pre-processing of the data. The aim of the pre-process is to replicate their behaviour analysis, so I can be sure that my data pre-processing is in line with theirs. First, I used a combination of sapply and table functions to check all coding numbers in the categorical variables columns. require(ggdmc) dat &lt;- fread(\"data race Study1TrialData.csv\") dplyr::tbl_df(dat) ## A tibble: 5,600 x 11 ## subject race0W1B object0NG1G conditionRaceObj conditionRace rt resp0DS1S ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 4 2 464 1 ## 2 1 1 0 2 2 658 0 ## 3 1 1 1 4 2 776 1 ## 4 1 0 1 3 1 646 1 ## 5 1 0 0 1 1 624 0 ## 6 1 0 1 3 1 518 1 ## 7 1 1 0 2 2 678 0 ## 8 1 0 1 3 1 511 1 ## 9 1 0 0 1 1 602 1 ## 10 1 1 1 4 2 808 1 ## ... with 5,590 more rows, and 4 more variables: diffusionRT &lt;dbl&gt;, ybin &lt;int&gt;, ## lowerLim &lt;dbl&gt;, upperLim &lt;dbl&gt; sapply(dat[, c(\"subject\", \"race0W1B\", \"object0NG1G\", \"conditionRaceObj\", \"conditionRace\", \"resp0DS1S\")], table) ## $subject ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 ## 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 ## 45 46 47 48 49 50 51 52 53 54 55 56 ## 100 100 100 100 100 100 100 100 100 100 100 100 ## ## $race0W1B ## ## 0 1 ## 2800 2800 ## ## $object0NG1G ## ## 0 1 ## 2800 2800 ## ## $conditionRaceObj ## ## 1 2 3 4 ## 1400 1400 1400 1400 ## ## $conditionRace ## ## 1 2 ## 2800 2800 ## ## $resp0DS1S ## ## 0 1 ## 2636 2796 Second, I relabeled their numerical coding to character strings, using ifelse and factor functions. In the “object0NG1G” for example, ifelse function finds “0” and converts it to “non”, meaning non-gun condition. Otherwise, it converts any numbers it found to “gun”, meaning gun condition. Because I have used table to check all available numbers, leaving all other number to else is OK. factor function converts the integer column (which will be interpreted as continuous variable) to categorical (i.e., nominal) variables. Next, I used the data.table way to remove the redundant columns, because I have reformatted them to follow our convention standard (e.g., using single uppercase letters referring to experimental factors). dat$S &lt;- factor(ifelse(dat$object0NG1G == 0, \"non\", \"gun\")) dat$RACE &lt;- factor(ifelse(dat$race0W1B == 0, \"white\", \"black\")) dat$R &lt;- factor(ifelse(dat$resp0DS1S == 0, \"not\", \"shoot\")) dat$RT &lt;- dat$rt 1e3 dat$s &lt;- factor(dat$subject) dat[, c(\"subject\", \"race0W1B\", \"object0NG1G\", \"conditionRaceObj\", \"conditionRace\", \"rt\", \"resp0DS1S\", \"diffusionRT\", \"ybin\", \"lowerLim\", \"upperLim\") := NULL] Real data sets often contain some abnormal responses, such as outliers, very slow, very quick, and wrong key responses. I used is.nan function to check whether the RT columns have this type of responses. is.nan returns a logical vector, indicating that if an element it found is “Not a number”, it will return FALSE, otherwise TRUE. I then added all elements in the vector to see how many TRUEs (1) are there. Logical TRUE in R is interpreted as 1, relative to logical FALSE, which is interpreted as 0. I found there are 168 such responses, which I removed them by a simple data.table syntax and stored the result as d. is.nan(dat$RT) ## FALSE FALSE FALSE FALSE FALSE FALSE ... sum(is.nan(dat$RT)) ## [1] 168 d &lt;- dat[!is.nan(dat$RT)] To help the calculation of the proportions of correct and error responses, I created two logical columns, C and error, to store whether a trial records a correct or error response. d$C &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", FALSE, NA)))) d$error &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", TRUE, NA)))) Next I examine how many trials in each experimental condition. This can be achieved by a simple data.table syntax. d[, .N, .(s, S, RACE)] # s S RACE N # 1: 1 gun black 25 # 2: 1 non black 25 # 3: 1 gun white 25 # 4: 1 non white 25 # 5: 2 non black 25 # --- # 220: 55 non white 25 # 221: 56 gun black 25 # 222: 56 gun white 25 # 223: 56 non black 25 # 224: 56 non white 25 Applying table function on the N column in the above resulting data table, I can check exactly the per-condition trial numbers. table(d[, .N, .(s, S, RACE)]$N) ## 19 21 22 23 24 25 ## 1 3 11 33 51 125 There are six trial numbers: 19, 21, 22, 23, 24, 25, with mostly subject-conditions combination (125) have 25 trials. nrow(d[, .N, .(s)]) unique(d$s) ## Fig. 3 source(\"~ rc data.analysis.R\") source(\"~ rc utils.R\") source(\"~ functions summarise.R\") d acc0 &lt;- summarySE(d, mv = \"error\", gvs = c(\"s\", \"RACE\", \"S\")) mrt0 &lt;- summarySE(d[C == TRUE], mv = \"RT\", gvs = c(\"s\", \"RACE\", \"S\")) ## Within se average across subjects for pc and nt figA &lt;- summarySEwithin(acc0, wvs = c(\"RACE\", \"S\"), mv = \"error\") figB &lt;- summarySEwithin(mrt0, wvs = c(\"RACE\", \"S\"), mv = \"RT\") names(figA) &lt;- c(\"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") names(figB) &lt;- c(\"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") head(figA) dplyr::tbl_df(figA) levels(figA$RACE) figA$RACE &lt;- factor(figA$RACE, levels = c(\"white\", \"black\"), labels = c(\"White\", \"Black\")) figA$S &lt;- factor(figA$S, levels = c(\"non\", \"gun\"), labels = c(\"Non-Gun\", \"Gun\")) # figA$gp &lt;- factor(paste(figA$CT, figA$S), # levels = c(\"safe non\", \"safe gun\", \"danger non\", \"danger gun\"), # labels = c(\"Neutral Non-Gun\", \"Neutral Gun\", \"Dangerous Non-Gun\", \"Dangerous Gun\")) figB$RACE &lt;- factor(figB$RACE, levels = c(\"white\", \"black\"), labels = c(\"White\", \"Black\")) figB$S &lt;- factor(figB$S, levels = c(\"non\", \"gun\"), labels = c(\"Non-Gun\", \"Gun\")) # figB$gp &lt;- factor(paste(figB$CT, figB$S), # levels = c(\"safe non\", \"safe gun\", \"danger non\", \"danger gun\"), # labels = c(\"Neutral Non-Gun\", \"Neutral Gun\", \"Dangerous Non-Gun\", \"Dangerous Gun\")) # figA$parameter &lt;- \"ER\" # figB$parameter &lt;- \"RT\" # fig7 &lt;- rbind(figA, figB) # head(fig7) # Error bars represent standard error of the mean p1 &lt;- ggplot(figA, aes(x = S, y = y, fill = RACE)) + geom_bar(position = position_dodge(), color = \"black\", stat=\"identity\") + geom_errorbar(aes(ymin = y - se, ymax = y + se), width=.1, position=position_dodge(.9)) + coord_cartesian(ylim = c(0, .20)) + scale_fill_manual(values = c(\"#FFFFFF\", \"#CCCCCC\")) + ylab(\"Error Rate\") + coord_cartesian(ylim = c(0, .08)) + # facet_grid(.~BC) + theme_bw() + theme(legend.position = c(.85, .75), strip.background = element_blank(), axis.title.y = element_text(size = 20), strip.text.x = element_text(size = 18), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_text(size = 18), axis.text.x = element_blank()) p2 &lt;- ggplot(figB, aes(x = S, y = y, fill = RACE)) + geom_bar(position = position_dodge(), color = \"black\", stat=\"identity\") + geom_errorbar(aes(ymin = y - se, ymax = y + se), width=.1, position=position_dodge(.9)) + scale_fill_manual(values = c(\"#FFFFFF\", \"#CCCCCC\")) + ylab(\"Correct Response Time (s)\") + coord_cartesian(ylim = c(.54, .65)) + # facet_grid(.~BC) + theme_bw() + theme(legend.position = \"none\", strip.text.x = element_blank(), axis.title.y = element_text(size = 20), axis.text.x = element_text(size = 18), axis.text.y = element_text(size = 18), axis.title.x = element_blank()) png(\"figs race fig3.png\", 800, 600) grid.arrange(p1, p2, ncol = 1) dev.off() save(dat, d, file = \"data race study1.rda\") load(\"data race study1.rda\") load(\"data race shoot-decision-recovery.rda\") study3_subset &lt;- study3[, c(\"s\", \"S\", \"RACE\", \"R\", \"RT\")] dplyr::tbl_df(study3_subset) ## A tibble: 12,033 x 5 ## s S RACE R RT ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 11 gun black not 0.753 ## 2 11 non white shoot 0.851 ## 3 11 gun black not 0.742 ## 4 11 non white shoot 0.636 ## 5 11 gun black shoot 0.644 ## 6 11 non black not 0.625 ## 7 11 non white shoot 0.889 ## 8 11 gun black not 0.597 ## 9 11 gun white not 0.724 ## 10 11 non white shoot 0.656 ## ... with 12,023 more rows To match the abbreviations used in the model object, I changed the “non”, and “gun” to “N” and “G” as well as “black” and “white” to “A” and “E”. study3_subset$S &lt;- factor(ifelse(study3_subset$S == “non”, “N”, “G”)) study3_subset$RACE &lt;- factor(ifelse(study3_subset$RACE == “black”, “A”, “E”)) Then I converted the data.table to data.frame, which was then bound to the model object. edat &lt;- data.frame(study3_subset); edmi &lt;- BindDataModel(edat, model) Next is just to repeat what I had done in the recovery study. path &lt;- \"data race Study3 DDM stimulus-threshold.rda\" ehsam &lt;- run(StartNewHypersamples(5e2, edmi, p.prior, pp.prior, 32), pm = .3, hpm = .3) ## 18 mins ehsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) save(model, p.prior, pp.prior, pop.prior, nsubject, ntrial, dat, dmi, hsam, npar, pop.mean, pop.scale, ps, study3, edat, edmi, ehsam, counter, file = path) I then set up an automatic fitting routine to fit the model until it converges. counter &lt;- 1 repeat { ehsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) save(model, p.prior, pp.prior, pop.prior, nsubject, ntrial, dat, dmi, hsam, npar, pop.mean, pop.scale, ps, study3, edat, edmi, ehsam, counter, file = path) rhats &lt;- hgelman(ehsam) counter &lt;- counter + 1 thin &lt;- thin * 2 if (all(rhats &lt; 1.1) || counter &gt; 1e2) break } Model Diagnosis Potential scale reduction factor (psrf) Effective sample sizes rhats &lt;- hgelman(ehsam) # Diagnosing theta for many participants separately # Diagnosing the hyper parameters, phi # hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.03 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 # 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 # 28 29 30 31 32 33 34 35 36 37 38 # 1.00 1.00 1.00 1.00 1.00 1.00 1.01 1.01 1.01 1.01 1.01 effectiveSize(ehsam, hyper = TRUE) # a.E.h1 a.A.h1 v.G.h1 v.N.h1 z.h1 sz.h1 sv.h1 t0.h1 a.E.h2 a.A.h2 v.G.h2 v.N.h2 # 2047 1985 1337 1819 1995 981 1444 2084 1724 1874 1314 1666 # z.h2 sz.h2 sv.h2 t0.h2 # 1936 1072 1267 1957 effectiveSize(ehsam, verbose = TRUE) # a.E a.A v.G v.N z sz sv t0 # MEAN 6295 5888 5464 6021 7029 5339 5895 6457 # SD 940 937 964 922 987 767 948 612 # MAX 7366 7202 6483 7403 7979 6783 7350 7372 # MIN 2042 1873 1922 2037 1922 1930 2147 3914 Trace plots for the log-posterior likelihood at the hyper level Trace plots for hyper parameters Posterior density plots for the hyper parameters Trace plots for the log-posterior likelihood at the data level Posterior density plots for the DDM parameters in the 38 participants The last figures were not presented here. They were printed separately in a pdf file for later checking. p1 &lt;- plot(ehsam, hyper = TRUE) p2 &lt;- plot(ehsam, hyper = TRUE, pll = FALSE) p3 &lt;- plot(ehsam, hyper = TRUE, pll = FALSE, den = TRUE) p4 &lt;- plot(ehsam) p5 &lt;- plot(ehsam, pll = FALSE, den = TRUE) Because this is not a parameter recovery study, the next step is to estimate the DDM parameters. In the race-threshold model, we, following Pleskac, Cesario and Johson’s (2017) hypothesis, expected to see a higher threshold (at the boundary separation parameter) for a black target than for a white target. One particular strength in the hierarchical modeling is that we can ask the question that whether this specific hypothesis happens at the population level, because the hierarchical modeling assumes the 38 participants in this study are just a small subset of people the researchers (pseudo-)randomly drew from a large population, presumably the entire population in U.S.A. This is in contrast to the fixed-effect model, which assumes each participant has her his own DDM mechanism of data generation. The following is how you may do these in ggdmc syntax. When entering hmean = TRUE, the summary function will calculate the average values for the hyper parameters. Similarly, the option, hci = TRUE triggers the calculation of credible interval at the hyper parameters. hest1 &lt;- summary(hsam, hyper = TRUE, hmean = TRUE) hest2 &lt;- summary(hsam, hyper = TRUE, hci = TRUE) # a.E.h1 a.A.h1 v.G.h1 v.N.h1 z.h1 sz.h1 sv.h1 t0.h1 # h1 1.57 2.60 4.11 3.00 0.48 0.29 0.96 0.22 # h2 0.53 0.86 0.52 0.68 0.11 0.18 0.52 0.05 # Random-effect model with multiple participants # L 2.5% 50% 97.5% S 2.5% 50% 97.5% # a.E 1.39 1.57 1.75 0.41 0.52 0.71 # a.A 2.32 2.60 2.89 0.67 0.85 1.14 # v.G 3.88 4.11 4.36 0.34 0.52 0.74 # v.N 2.75 3.00 3.25 0.50 0.67 0.90 # z 0.44 0.48 0.51 0.09 0.11 0.14 # sz 0.09 0.31 0.39 0.10 0.17 0.32 # sv 0.58 0.98 1.18 0.33 0.50 0.85 # t0 0.20 0.22 0.23 0.04 0.05 0.07 The results support the hypothesis that black targets result in higher decision threshold than white targets (a.E.h1 = 1.57 [1.39 - 1.75] &lt; a.A.h1 = 2.60 [2.32 - 2.89]). Note I can make this claim because the credible intervals for these two conditions are not overlapped. Also, as expected, the drift rate for gun objects is faster than that for the non-gun objects (v.G.h1 = 4.11 [3.88 - 4.36] &gt; v.N.h1 = 3.00 [2.75 - 3.25]). The finding of boundary separation here differs from the result in Pleskac, Cesario, &amp; Johnson (2017, Fig. 9; also Threshold separation section on page 18). They did not find threshold difference, perhaps because they analyzed four factors, resulting in small trial numbers in each condition. I showed R codes below to print this information. s: subject S: stimulus, gun vs. nongun B: blur or clear object CT: context, danger or neutral context RACE: black vs. white targets study3[, .N, .(s, S, B, CT, RACE)] ## s S B CT RACE N ## 1: 11 gun blur safe black 22 ## 2: 11 non blur safe white 23 ## 3: 11 gun clear safe black 19 ## 4: 11 non clear safe white 18 ## 5: 11 non clear safe black 21 ## --- ## 604: 348 gun clear danger black 17 ## 605: 348 non clear danger white 28 ## 606: 348 gun blur danger white 20 ## 607: 348 non clear danger black 16 ## 608: 348 gun blur danger black 23 &gt; range(study3[, .N, .(s, S, B, CT, RACE)]$N) ## [1] 6 33 In case you may be interested, I listed the estimates for each participants below. Not every participant has a higher threshold for black targets than white targets. ests &lt;- summary(hsam) round(ests, 2) ## ## a.E a.A v.G v.N z sz sv t0 ## 1 1.30 3.38 4.60 2.84 0.40 0.18 1.02 0.17 ## 2 1.25 2.66 3.16 2.99 0.45 0.52 1.97 0.17 ## 3 2.44 3.82 3.61 2.88 0.50 0.42 0.83 0.16 ## 4 2.70 2.10 3.87 2.84 0.49 0.37 0.71 0.16 ## 5 1.92 2.05 4.13 3.16 0.46 0.23 1.34 0.20 ## 6 1.59 2.56 4.10 3.40 0.41 0.27 1.35 0.26 ## 7 1.13 2.91 4.04 3.11 0.42 0.45 0.87 0.21 ## 8 1.83 2.97 4.16 2.69 0.48 0.36 0.62 0.21 ## 9 1.49 1.32 4.37 2.19 0.38 0.31 1.21 0.22 ## 10 1.28 2.88 4.80 1.94 0.48 0.14 1.31 0.27 ## 11 0.99 1.26 4.54 3.27 0.59 0.25 1.05 0.19 ## 12 1.23 4.24 4.42 3.53 0.41 0.22 1.11 0.19 ## 13 1.50 2.17 3.69 3.13 0.51 0.41 1.03 0.17 ## 14 1.90 2.07 4.20 2.59 0.42 0.50 0.63 0.26 ## 15 1.57 3.35 4.33 2.47 0.61 0.21 1.04 0.32 ## 16 1.55 2.89 3.88 3.43 0.33 0.28 1.06 0.21 ## 17 1.22 2.58 4.81 2.95 0.45 0.30 0.41 0.28 ## 18 2.23 0.31 4.37 2.74 0.33 0.49 1.21 0.18 ## 19 1.55 2.77 4.13 3.00 0.40 0.51 0.44 0.22 ## 20 1.20 2.61 4.50 4.04 0.42 0.50 0.45 0.25 ## 21 0.74 2.68 4.41 2.39 0.47 0.25 0.81 0.19 ## 22 1.78 2.89 3.42 3.97 0.64 0.17 2.02 0.14 ## 23 2.25 2.60 3.99 4.02 0.55 0.24 0.96 0.18 ## 24 1.76 2.92 4.69 2.70 0.70 0.44 0.44 0.17 ## 25 0.91 3.45 3.98 2.52 0.72 0.15 0.88 0.32 ## 26 1.33 3.12 3.72 3.56 0.29 0.55 1.17 0.25 ## 27 1.42 2.96 3.72 3.18 0.55 0.20 1.34 0.28 ## 28 1.25 1.95 4.06 1.99 0.45 0.28 1.51 0.30 ## 29 1.21 2.39 4.22 3.69 0.55 0.42 0.73 0.19 ## 30 2.45 2.01 4.71 2.54 0.39 0.19 1.31 0.21 ## 31 1.27 1.71 4.34 3.81 0.45 0.24 0.85 0.17 ## 32 0.82 2.61 3.98 3.24 0.56 0.33 0.87 0.25 ## 33 1.80 2.42 3.51 3.92 0.44 0.21 1.44 0.17 ## 34 1.93 2.49 3.97 2.70 0.62 0.37 1.42 0.32 ## 35 1.42 1.39 3.93 1.60 0.42 0.34 1.44 0.21 ## 36 2.53 4.26 3.60 2.60 0.32 0.25 0.58 0.17 ## 37 1.08 2.86 4.18 3.35 0.52 0.44 0.70 0.22 ## 38 2.06 3.61 4.24 3.11 0.63 0.34 0.61 0.22 ## Mean 1.58 2.61 4.12 3.00 0.48 0.32 1.02 0.22 Reference Pleskac, T.J., Cesario, J. &amp; Johnson, D.J. (2017). How race affects evidence accumulation during the decision to shoot. Psychonomic Bulletin &amp; Review, 1-30. https: doi.org 10.3758 s13423-017-1369-6"
					}

					
				
			
		
			
				
					,
					

					"sampling-crossover": {
						"id": "sampling-crossover",
						"title": "Crossover",
						"category": "",
						"url": " /sampling/crossover/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-demc": {
						"id": "sampling-demc",
						"title": "Differential Evolution Monte Carlo",
						"category": "",
						"url": " /sampling/demc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-demcmc": {
						"id": "sampling-demcmc",
						"title": "Differential Evolution Markov Chain Monte Carlo",
						"category": "",
						"url": " /sampling/demcmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-dgmc": {
						"id": "sampling-dgmc",
						"title": "Distributed Genetic Monte Carlo",
						"category": "",
						"url": " /sampling/dgmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-genetic": {
						"id": "sampling-genetic",
						"title": "Population-based Monte Carlo",
						"category": "",
						"url": " /sampling/genetic/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-migration": {
						"id": "sampling-migration",
						"title": "Migration",
						"category": "",
						"url": " /sampling/migration/",
						"content": "Migration operator is one crucial operator in the distributed genetic algorithm (Tanese, 1989; Hu &amp; Tsai, 2005), originated from the genetic algorithm (Holland, 1975; Goldberg, 1989). The algorithm uses a similar scheme, like chromosomes exchange gene information. Migration in MCMC computation uses the same method as in random-walk Metropolis to propose a new proposal, which is then subjected to the Metropolis decision step to accept or reject as a valid sample from a target distribution. The method of migration operator to propose a proposal is that it adds random noise vector onto a “current” parameter vector as a new proposal."
					}

					
				
			
		
			
				
					,
					

					"sampling-mutation": {
						"id": "sampling-mutation",
						"title": "Mutation",
						"category": "",
						"url": " /sampling/mutation/",
						"content": ""
					}

					
				
			
		
	};
</script>
<script src="/scripts/lunr.min.js"></script>
<script src="/scripts/search.js"></script>

			</article>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});

			
		</script>

		  
		</script>
	</body>
</html>
